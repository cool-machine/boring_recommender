{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to Python's path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import from metrics module\n",
    "from metrics import evaluate_recommendations_df, evaluate_all_models, format_evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 3. Popularity/Recency on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gather": {
     "logged": 1745761833630
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "# ── CONFIG ────────────────────────────────────────────────────\n",
    "TRAIN_PATH        = \"../datasets/train_clicks.parquet\"\n",
    "VAL_PATH          = \"../datasets/valid_clicks.parquet\"\n",
    "TEST_PATH         = \"../datasets/test_clicks.parquet\"\n",
    "META_PATH         = \"../datasets/articles_metadata.csv\"   # or your parquet copy\n",
    "HALF_LIFE_DAYS    = [1, 3, 7, 14]\n",
    "BETA_VALUES       = [0.0, 0.05, 0.1, 0.2]\n",
    "FRESH_WINDOW_DAYS = 1\n",
    "TOP_M             = 500\n",
    "RECALL_K          = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1745761795574
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (2857109, 12), Val: (65536, 12), Test: (65536, 12)\n"
     ]
    }
   ],
   "source": [
    "# 1) Load train / val / test\n",
    "train_df = pd.read_parquet(TRAIN_PATH)\n",
    "val_df   = pd.read_parquet(VAL_PATH)\n",
    "test_df  = pd.read_parquet(TEST_PATH)\n",
    "\n",
    "# ensure timestamps are datetime\n",
    "for df in (train_df, val_df, test_df):\n",
    "    df[\"click_timestamp\"] = pd.to_datetime(df[\"click_timestamp\"], unit=\"ms\")\n",
    "\n",
    "print(f\"Train: {train_df.shape}, Val: {val_df.shape}, Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1745761840099
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364047, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>created_at_ts</th>\n",
       "      <th>publisher_id</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-13 05:53:39</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-07-14 12:45:36</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-08-22 00:35:06</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-08-19 17:11:53</td>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-08-03 13:06:11</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  category_id       created_at_ts  publisher_id  words_count\n",
       "0           0            0 2017-12-13 05:53:39             0          168\n",
       "1           1            1 2014-07-14 12:45:36             0          189\n",
       "2           2            1 2014-08-22 00:35:06             0          250\n",
       "3           3            1 2014-08-19 17:11:53             0          230\n",
       "4           4            1 2014-08-03 13:06:11             0          162"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Load metadata\n",
    "dtypes = {\n",
    "        \"article_id\": \"uint32\",\n",
    "        \"category_id\": \"uint16\",\n",
    "        \"publisher_id\": \"uint8\",\n",
    "        \"words_count\": \"uint16\"\n",
    "    }\n",
    "\n",
    "articles = pd.read_csv(META_PATH,dtype=dtypes)\n",
    "\n",
    "articles[\"created_at_ts\"] = pd.to_datetime(articles[\"created_at_ts\"], unit=\"ms\")\n",
    "print(articles.shape)\n",
    "articles.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1745762736320
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "def compute_scores(train_df, articles, half_life_days, beta, fresh_window_days, top_m=500):\n",
    "    # 1) global reference time\n",
    "    now = train_df[\"click_timestamp\"].max()\n",
    "    # 2) decay constant (per second)\n",
    "    λ = np.log(2) / pd.Timedelta(days=half_life_days).total_seconds()\n",
    "    # 3) per-click weights\n",
    "    ages    = (now - train_df[\"click_timestamp\"]).dt.total_seconds()\n",
    "    weights = np.exp(-λ * ages)\n",
    "    # 4) aggregate per article\n",
    "    pop = (\n",
    "        train_df.assign(weight=weights)\n",
    "                .groupby(\"click_article_id\")[\"weight\"]\n",
    "                .sum()\n",
    "                .rename(\"pop_score\")\n",
    "                .to_frame()\n",
    "    )\n",
    "    # 5) join publication times\n",
    "    pop = pop.join(\n",
    "        articles.set_index(\"article_id\")[[\"created_at_ts\"]],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    # 6) apply freshness boost\n",
    "    is_fresh = (now - pop[\"created_at_ts\"]) <= pd.Timedelta(days=fresh_window_days)\n",
    "    pop[\"final_score\"] = pop[\"pop_score\"] * (1 + beta * is_fresh.astype(float))\n",
    "    # 7) build cleaned candidates DF\n",
    "    top_series = pop[\"final_score\"].nlargest(top_m)\n",
    "    cands = pd.DataFrame({\n",
    "        \"article_id\": top_series.index.astype(int),\n",
    "        \"final_score\": top_series.values\n",
    "    })\n",
    "    return cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry 1/16: h=1, β=0.0\n",
      " → Recall@10: 0.0714\n",
      " → precision@10: 0.0071\n",
      " → f1@10: 0.0130\n",
      " → ndcg@10: 0.0210\n",
      "\n",
      "Entry 2/16: h=1, β=0.05\n",
      " → Recall@10: 0.0714\n",
      " → precision@10: 0.0071\n",
      " → f1@10: 0.0130\n",
      " → ndcg@10: 0.0210\n",
      "\n",
      "Entry 3/16: h=1, β=0.1\n",
      " → Recall@10: 0.0714\n",
      " → precision@10: 0.0071\n",
      " → f1@10: 0.0130\n",
      " → ndcg@10: 0.0210\n",
      "\n",
      "Entry 4/16: h=1, β=0.2\n",
      " → Recall@10: 0.0714\n",
      " → precision@10: 0.0071\n",
      " → f1@10: 0.0130\n",
      " → ndcg@10: 0.0210\n",
      "\n",
      "Entry 5/16: h=3, β=0.0\n",
      " → Recall@10: 0.1470\n",
      " → precision@10: 0.0147\n",
      " → f1@10: 0.0267\n",
      " → ndcg@10: 0.0641\n",
      "\n",
      "Entry 6/16: h=3, β=0.05\n",
      " → Recall@10: 0.1470\n",
      " → precision@10: 0.0147\n",
      " → f1@10: 0.0267\n",
      " → ndcg@10: 0.0641\n",
      "\n",
      "Entry 7/16: h=3, β=0.1\n",
      " → Recall@10: 0.1470\n",
      " → precision@10: 0.0147\n",
      " → f1@10: 0.0267\n",
      " → ndcg@10: 0.0641\n",
      "\n",
      "Entry 8/16: h=3, β=0.2\n",
      " → Recall@10: 0.1470\n",
      " → precision@10: 0.0147\n",
      " → f1@10: 0.0267\n",
      " → ndcg@10: 0.0641\n",
      "\n",
      "Entry 9/16: h=7, β=0.0\n",
      " → Recall@10: 0.0582\n",
      " → precision@10: 0.0058\n",
      " → f1@10: 0.0106\n",
      " → ndcg@10: 0.0297\n",
      "\n",
      "Entry 10/16: h=7, β=0.05\n",
      " → Recall@10: 0.0582\n",
      " → precision@10: 0.0058\n",
      " → f1@10: 0.0106\n",
      " → ndcg@10: 0.0297\n",
      "\n",
      "Entry 11/16: h=7, β=0.1\n",
      " → Recall@10: 0.0582\n",
      " → precision@10: 0.0058\n",
      " → f1@10: 0.0106\n",
      " → ndcg@10: 0.0297\n",
      "\n",
      "Entry 12/16: h=7, β=0.2\n",
      " → Recall@10: 0.0582\n",
      " → precision@10: 0.0058\n",
      " → f1@10: 0.0106\n",
      " → ndcg@10: 0.0297\n",
      "\n",
      "Entry 13/16: h=14, β=0.0\n",
      " → Recall@10: 0.0444\n",
      " → precision@10: 0.0044\n",
      " → f1@10: 0.0081\n",
      " → ndcg@10: 0.0205\n",
      "\n",
      "Entry 14/16: h=14, β=0.05\n",
      " → Recall@10: 0.0444\n",
      " → precision@10: 0.0044\n",
      " → f1@10: 0.0081\n",
      " → ndcg@10: 0.0205\n",
      "\n",
      "Entry 15/16: h=14, β=0.1\n",
      " → Recall@10: 0.0444\n",
      " → precision@10: 0.0044\n",
      " → f1@10: 0.0081\n",
      " → ndcg@10: 0.0205\n",
      "\n",
      "Entry 16/16: h=14, β=0.2\n",
      " → Recall@10: 0.0444\n",
      " → precision@10: 0.0044\n",
      " → f1@10: 0.0081\n",
      " → ndcg@10: 0.0205\n",
      "\n",
      "Best hyperparameters:\n",
      " half_life_days    3.000000\n",
      "beta              0.000000\n",
      "recall@10         0.147034\n",
      "precision@10      0.014703\n",
      "f1@10             0.026733\n",
      "ndcg@10           0.064105\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "results = []\n",
    "for counter, (h, beta) in enumerate(product(HALF_LIFE_DAYS, BETA_VALUES)):\n",
    "    print(f\"Entry {counter+1}/{len(HALF_LIFE_DAYS)*len(BETA_VALUES)}: h={h}, β={beta}\")\n",
    "    \n",
    "    # Generate recommendations using your existing function\n",
    "    cands = compute_scores(train_df, articles, h, beta, FRESH_WINDOW_DAYS, top_m=TOP_M)\n",
    "    \n",
    "    # Convert to DataFrame format if not already\n",
    "    if not isinstance(cands, pd.DataFrame):\n",
    "        cands_df = pd.DataFrame({\"article_id\": cands})\n",
    "    else:\n",
    "        cands_df = cands\n",
    "    \n",
    "    # Evaluate with the new metrics function\n",
    "    metrics = evaluate_recommendations_df(\n",
    "        recommendations_df=cands_df,\n",
    "        test_df=val_df,\n",
    "        articles_df=articles,\n",
    "        k_values=[RECALL_K]  # Use your existing RECALL_K\n",
    "    )\n",
    "    \n",
    "    # Get the recall value for your existing K\n",
    "    r = metrics.get(f'recall@{RECALL_K}', 0)\n",
    "    \n",
    "    # Store results in the same format as before\n",
    "    result_entry = {\n",
    "        \"half_life_days\": h,\n",
    "        \"beta\": beta,\n",
    "        f\"recall@{RECALL_K}\": r\n",
    "    }\n",
    "    \n",
    "    # Add other metrics if you want them\n",
    "    for metric in ['precision', 'f1', 'ndcg']:\n",
    "        metric_key = f'{metric}@{RECALL_K}'\n",
    "        if metric_key in metrics:\n",
    "            result_entry[metric_key] = metrics[metric_key]\n",
    "    \n",
    "    results.append(result_entry)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\" → Recall@{RECALL_K}: {r:.4f}\")\n",
    "    \n",
    "    # Optionally print other metrics\n",
    "    for metric in ['precision', 'f1', 'ndcg']:\n",
    "        metric_key = f'{metric}@{RECALL_K}'\n",
    "        if metric_key in metrics:\n",
    "            print(f\" → {metric_key}: {metrics[metric_key]:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Create DataFrame and find best parameters as before\n",
    "df_res = pd.DataFrame(results)\n",
    "best = df_res.sort_values(f\"recall@{RECALL_K}\", ascending=False).iloc[0]\n",
    "print(\"Best hyperparameters:\\n\", best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3.2 Combine training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1745764109781
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cell: Final Evaluation on Test Set\n",
    "\n",
    "# 1) Load splits (if not already in memory)\n",
    "train_df = pd.read_parquet(\"../datasets/train_clicks.parquet\")\n",
    "val_df   = pd.read_parquet(\"../datasets/valid_clicks.parquet\")\n",
    "test_df  = pd.read_parquet(\"../datasets/test_clicks.parquet\")\n",
    "\n",
    "# Ensure timestamps are datetime\n",
    "for df in (train_df, val_df, test_df):\n",
    "    df[\"click_timestamp\"] = pd.to_datetime(df[\"click_timestamp\"], unit=\"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1745764140699
    }
   },
   "outputs": [],
   "source": [
    "# 2) Load metadata (if not already in memory)\n",
    "articles = pd.read_csv(\n",
    "    \"../datasets/articles_metadata.csv\",\n",
    "    dtype={\n",
    "        \"article_id\": \"uint32\",\n",
    "        \"category_id\": \"uint16\",\n",
    "        \"publisher_id\": \"uint8\",\n",
    "        \"words_count\": \"uint16\"\n",
    "    }\n",
    ")\n",
    "articles[\"created_at_ts\"] = pd.to_datetime(articles[\"created_at_ts\"], unit=\"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gather": {
     "logged": 1745764160505
    }
   },
   "outputs": [],
   "source": [
    "# 3) Combine train + validation\n",
    "train_plus_val = pd.concat([train_df, val_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_scores(train_df, articles, half_life_days, beta, fresh_window_days, top_m=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating final model on test set...\n",
      "\n",
      "===== Final Test Results =====\n",
      "Model: Popularity-Recency (h=3, β=0)\n",
      "-----------------------------\n",
      "\n",
      "At K = 5:\n",
      "  Recall    : 0.0915\n",
      "  Precision : 0.0183\n",
      "  F1        : 0.0305\n",
      "  Ndcg      : 0.0500\n",
      "  Novelty   : 8.0724\n",
      "\n",
      "At K = 10:\n",
      "  Recall    : 0.1211\n",
      "  Precision : 0.0121\n",
      "  F1        : 0.0220\n",
      "  Ndcg      : 0.0597\n",
      "  Novelty   : 7.9813\n",
      "\n",
      "At K = 20:\n",
      "  Recall    : 0.1819\n",
      "  Precision : 0.0091\n",
      "  F1        : 0.0173\n",
      "  Ndcg      : 0.0746\n",
      "  Novelty   : 8.4886\n",
      "\n",
      "At K = 50:\n",
      "  Recall    : 0.2868\n",
      "  Precision : 0.0057\n",
      "  F1        : 0.0112\n",
      "  Ndcg      : 0.0953\n",
      "  Novelty   : 9.0322\n",
      "\n",
      "===== Summary Table =====\n",
      "      Recall  Precision      F1  NDCG  Novelty    Ndcg\n",
      "K                                                     \n",
      "5.0   0.0915     0.0183  0.0305   NaN   8.0724  0.0500\n",
      "10.0  0.1211     0.0121  0.0220   NaN   7.9813  0.0597\n",
      "20.0  0.1819     0.0091  0.0173   NaN   8.4886  0.0746\n",
      "50.0  0.2868     0.0057  0.0112   NaN   9.0322  0.0953\n",
      "\n",
      "Original Test Recall@10: 0.1211\n"
     ]
    }
   ],
   "source": [
    "# 5) Evaluate on test set\n",
    "print(\"Evaluating final model on test set...\")\n",
    "\n",
    "# Generate recommendations with your best parameters\n",
    "cands = compute_scores(train_plus_val, articles, half_life_days=3, beta=0, fresh_window_days=FRESH_WINDOW_DAYS, top_m=TOP_M)\n",
    "\n",
    "# Convert to DataFrame format if not already\n",
    "if not isinstance(cands, pd.DataFrame):\n",
    "    cands_df = pd.DataFrame({\"article_id\": cands})\n",
    "else:\n",
    "    cands_df = cands\n",
    "\n",
    "# Calculate item popularity for novelty metric\n",
    "item_popularity = train_df['click_article_id'].value_counts().to_dict()\n",
    "total_interactions = len(train_df)\n",
    "\n",
    "# Evaluate with all metrics\n",
    "metrics = evaluate_recommendations_df(\n",
    "    recommendations_df=cands_df,\n",
    "    test_df=test_df,\n",
    "    articles_df=articles,\n",
    "    k_values=[5, 10, 20, 50],  # Multiple K values\n",
    "    item_popularity=item_popularity,\n",
    "    total_interactions=total_interactions\n",
    ")\n",
    "\n",
    "# Print all metrics in a formatted way\n",
    "print(\"\\n===== Final Test Results =====\")\n",
    "print(f\"Model: Popularity-Recency (h=3, β=0)\")\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "# Group metrics by type and k value\n",
    "metric_types = ['recall', 'precision', 'f1', 'ndcg', 'novelty']\n",
    "k_values = [5, 10, 20, 50]\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nAt K = {k}:\")\n",
    "    for metric in metric_types:\n",
    "        metric_key = f'{metric}@{k}'\n",
    "        if metric_key in metrics:\n",
    "            print(f\"  {metric.capitalize():10}: {metrics[metric_key]:.4f}\")\n",
    "\n",
    "# Create a summary DataFrame for easy visualization\n",
    "import pandas as pd\n",
    "summary = pd.DataFrame({\n",
    "    'K': [],\n",
    "    'Recall': [],\n",
    "    'Precision': [],\n",
    "    'F1': [],\n",
    "    'NDCG': [],\n",
    "    'Novelty': []\n",
    "})\n",
    "\n",
    "for k in k_values:\n",
    "    row = {'K': k}\n",
    "    for metric in metric_types:\n",
    "        metric_key = f'{metric}@{k}'\n",
    "        if metric_key in metrics:\n",
    "            row[metric.capitalize()] = metrics[metric_key]\n",
    "    summary = pd.concat([summary, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# Display the summary table\n",
    "print(\"\\n===== Summary Table =====\")\n",
    "print(summary.set_index('K').round(4))\n",
    "\n",
    "# Highlight the original metric for comparison\n",
    "print(f\"\\nOriginal Test Recall@{RECALL_K}: {metrics[f'recall@{RECALL_K}']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python (p9-recsys)",
   "language": "python",
   "name": "p9-recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
