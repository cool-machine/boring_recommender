{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee01fe96-a3f6-4f86-b22c-dda92d377e6e",
   "metadata": {},
   "source": [
    "# 1. Item to Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1055b5a-09dc-459f-bd89-6c4a5a13b719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# ─── 1) Start Spark Session ─────────────────────────────────────────────────────\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"ItemItemCF\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")   # tune shuffle parallelism\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# A) Extract only the two columns we need\n",
    "interactions = pd.read_parquet(\n",
    "    \"../datasets/train_clicks.parquet\",\n",
    "    columns=[\"user_id\", \"click_article_id\"],\n",
    "    engine=\"pyarrow\"\n",
    ")\n",
    "\n",
    "# Rename for clarity\n",
    "interactions = interactions.rename(columns={\"click_article_id\":\"item_id\"})\n",
    "\n",
    "# Write back out, coalescing to a single file\n",
    "interactions.to_parquet(\n",
    "    \"../datasets/train_interactions.parquet\",\n",
    "    index=False,\n",
    "    engine=\"pyarrow\",\n",
    "    coerce_timestamps=\"ms\",            # force any timestamps → ms\n",
    "    allow_truncated_timestamps=True    # just in case\n",
    ")\n",
    "print(\"Wrote minimal interactions to ../datasets/train_interactions.parquet\")\n",
    "\n",
    "# ─── B) Extract minimal validation hold‑out ─────────────────────────────────────\n",
    "val = pd.read_parquet(\n",
    "    \"../datasets/valid_clicks.parquet\",\n",
    "    columns=[\"user_id\", \"click_article_id\"],\n",
    "    engine=\"pyarrow\"\n",
    ").rename(columns={\"click_article_id\": \"true_item\"})\n",
    "\n",
    "val.to_parquet(\n",
    "    \"../datasets/valid_interactions.parquet\",\n",
    "    index=False,\n",
    "    engine=\"pyarrow\",\n",
    "    coerce_timestamps=\"ms\",\n",
    "    allow_truncated_timestamps=True\n",
    ")\n",
    "print(\"Wrote minimal interactions to ../datasets/valid_interactions.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d7964c-19e0-4b1f-bb65-8d3c0e90228a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ─── 2) Configuration ────────────────────────────────────────────────────────────\n",
    "TRAIN_FP     = \"../datasets/train_clicks.parquet\"\n",
    "NEIGHBORS_FP = \"../datasets/item_neighbors.parquet\"\n",
    "TOP_K_NEIGH  = 50   # number of neighbors to keep per item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9bac9f-ca7f-470a-9be0-6f561a946a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ─── 3) Load only the minimal interactions DataFrame ────────────────────────────\n",
    "interactions = spark.read.parquet(\"../datasets/train_interactions.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6935580-3c63-4af7-8713-68916a1bd022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession, Window\n",
    "# import pyspark.sql.functions as F\n",
    "\n",
    "# # ─── 1) Start Spark Session ─────────────────────────────────────────────────────\n",
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"ItemItemCF\")\n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"200\")   # tune shuffle parallelism\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# # ─── 2) Configuration ────────────────────────────────────────────────────────────\n",
    "# TRAIN_FP     = \"../datasets/train_clicks.parquet\"\n",
    "# NEIGHBORS_FP = \"../datasets/item_neighbors.parquet\"\n",
    "# TOP_K_NEIGH  = 50   # number of neighbors to keep per item\n",
    "\n",
    "# # ─── 3) Load train clicks DataFrame ─────────────────────────────────────────────\n",
    "# clicks = spark.read.parquet(TRAIN_FP)\n",
    "\n",
    "# # Keep only the fields we need for CF\n",
    "# interactions = (\n",
    "#     clicks\n",
    "#     .select(\"user_id\", F.col(\"click_article_id\").alias(\"item_id\"))\n",
    "#     .distinct()  # one interaction per user‐item\n",
    "# )\n",
    "\n",
    "# ─── 4) Compute total clicks per item for normalization ─────────────────────────\n",
    "item_counts = (\n",
    "    interactions\n",
    "    .groupBy(\"item_id\")\n",
    "    .agg(F.count(\"*\").alias(\"n_i\"))\n",
    ")\n",
    "\n",
    "# ─── 5) Generate co‐click counts via self‐join on user_id ────────────────────────\n",
    "pairs = (\n",
    "    interactions.alias(\"a\")\n",
    "    .join(interactions.alias(\"b\"), on=\"user_id\")\n",
    "    .where(F.col(\"a.item_id\") < F.col(\"b.item_id\"))\n",
    "    .groupBy(\"a.item_id\", \"b.item_id\")\n",
    "    .agg(F.count(\"*\").alias(\"co_count\"))\n",
    ")\n",
    "\n",
    "# ─── 6) Join item counts to compute cosine similarity ───────────────────────────\n",
    "pairs = (\n",
    "    pairs\n",
    "    # join on 'a' counts\n",
    "    .join(\n",
    "        item_counts.withColumnRenamed(\"item_id\", \"i\").withColumnRenamed(\"n_i\", \"n_i\"),\n",
    "        pairs[\"a.item_id\"] == F.col(\"i\")\n",
    "    )\n",
    "    .drop(\"i\")\n",
    "    # join on 'b' counts\n",
    "    .join(\n",
    "        item_counts.withColumnRenamed(\"item_id\", \"j\").withColumnRenamed(\"n_i\", \"n_j\"),\n",
    "        pairs[\"b.item_id\"] == F.col(\"j\")\n",
    "    )\n",
    "    .drop(\"j\")\n",
    "    # cosine similarity = co_count / sqrt(n_i * n_j)\n",
    "    .withColumn(\"sim\", F.col(\"co_count\") / F.sqrt(F.col(\"n_i\") * F.col(\"n_j\")))\n",
    ")\n",
    "\n",
    "# ─── 7) For each item, keep top‐TOP_K_NEIGH neighbors ────────────────────────────\n",
    "window_spec = Window.partitionBy(\"a.item_id\").orderBy(F.col(\"sim\").desc())\n",
    "\n",
    "item_neighbors = (\n",
    "    pairs\n",
    "    .withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "    .filter(F.col(\"rank\") <= TOP_K_NEIGH)\n",
    "    .select(\n",
    "        F.col(\"a.item_id\").alias(\"item_id\"),\n",
    "        F.col(\"b.item_id\").alias(\"neighbor_id\"),\n",
    "        \"sim\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# ─── 8) Persist the neighbor table ──────────────────────────────────────────────\n",
    "item_neighbors.write.mode(\"overwrite\").parquet(NEIGHBORS_FP)\n",
    "\n",
    "# ─── 9) Stop Spark ──────────────────────────────────────────────────────────────\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fef133-ded5-4c62-b440-568f9bb57f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170c924-fbfc-4b74-9d09-b149ee30f5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1971ad2d-7a13-48af-b3b2-9062d9a2d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from a terminal or notebook Python\n",
    "!rm -rf \"../datasets/cf_item_recs_top5\"\n",
    "!rm -rf \"../datasets/cf_item_recs_top10\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b00e9f-e77a-4626-a9eb-37f9be2a5f63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82df0b8-dcfd-4854-b4e8-d3a96c8a774c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# ─── (Re)Start Spark ─────────────────────────────────────────────────────────────\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"ItemItemCF_and_Eval\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .config(\"spark.driver.memory\",      \"32g\")    # bump driver JVM to 32 GB\n",
    "    .config(\"spark.executor.memory\",    \"32g\")    # (in local mode same JVM)\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Now you can read the Parquet files\n",
    "train_int = spark.read.parquet(\"../datasets/train_interactions.parquet\")\n",
    "val       = spark.read.parquet(\"../datasets/valid_interactions.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6833e41-2259-43ce-8c54-10388a9656db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# # ─── 1) Read in the minimal train interactions ──────────────────────────────────\n",
    "# #    (we wrote this earlier via pandas to drop timestamps)\n",
    "# train_int = spark.read.parquet(\"../datasets/train_interactions.parquet\")  \n",
    "# # schema: (user_id: long, item_id: long)\n",
    "\n",
    "# ─── 2) Read the precomputed item-neighbors ─────────────────────────────────────\n",
    "neighbors = spark.read.parquet(\"../datasets/item_neighbors.parquet\")\n",
    "# schema: (item_id: long, neighbor_id: long, sim: double)\n",
    "\n",
    "# # ─── 3) Read your validation hold-out (one click per user) ─────────────────────\n",
    "# val = (\n",
    "#     spark.read.parquet(\"../datasets/valid_clicks.parquet\")\n",
    "#          .select(\"user_id\", F.col(\"click_article_id\").alias(\"true_item\"))\n",
    "# )\n",
    "\n",
    "# ─── 4) Remove the hold-out from your training set (just in case) ──────────────\n",
    "train_minus_val = train_int.join(val, on=\"user_id\", how=\"left_anti\")\n",
    "\n",
    "# ─── 5) Explode each user’s clicks into neighbor candidates ─────────────────────\n",
    "cands = (\n",
    "    train_minus_val\n",
    "    .join(neighbors, train_minus_val.item_id == neighbors.item_id)\n",
    "    .select(\n",
    "      \"user_id\",\n",
    "      neighbors.neighbor_id.alias(\"candidate\"),\n",
    "      \"sim\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# ─── 6) Aggregate scores per (user, candidate) ─────────────────────────────────\n",
    "scores = (\n",
    "    cands\n",
    "    .groupBy(\"user_id\", \"candidate\")\n",
    "    .agg(F.sum(\"sim\").alias(\"score\"))\n",
    ")\n",
    "\n",
    "# ─── 7) Filter out anything the user already clicked in TRAIN  ──────────────────\n",
    "scores_filtered = (\n",
    "    scores\n",
    "    .join(train_minus_val,\n",
    "          (scores.user_id == train_minus_val.user_id) &\n",
    "          (scores.candidate == train_minus_val.item_id),\n",
    "          how=\"left_anti\")\n",
    ")\n",
    "\n",
    "# ─── 9) Rank and take top-5 / top-10 ────────────────────────────────────────────\n",
    "window = Window.partitionBy(\"user_id\").orderBy(F.col(\"score\").desc())\n",
    "\n",
    "recs = scores_filtered.withColumn(\"rank\", F.row_number().over(window))\n",
    "\n",
    "top5  = recs.filter(\"rank <= 5\")\n",
    "top10 = recs.filter(\"rank <= 10\")\n",
    "\n",
    "# ─── 10) Persist your recommendations ───────────────────────────────────────────\n",
    "NUM_PARTS = 200\n",
    "top5 \\\n",
    "    .repartition(NUM_PARTS) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"../datasets/cf_item_recs_top5.parquet\")\n",
    "\n",
    "top10 \\\n",
    "    .repartition(NUM_PARTS) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"../datasets/cf_item_recs_top10.parquet\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41df1d8b-c422-4e4c-bc36-040f5d71fb56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeadf9a4-55b1-4a61-a589-6fb2e7e1f792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# ─── 1) (Re)start Spark ─────────────────────────────────────────────────────────\n",
    "spark = SparkSession.builder.appName(\"EvalRecall\").getOrCreate()\n",
    "\n",
    "# ─── 2) Load validation true-item and CF recs ─────────────────────────────────\n",
    "val   = spark.read.parquet(\"../datasets/valid_interactions.parquet\")\n",
    "recs5 = spark.read.parquet(\n",
    "    \"../datasets/cf_item_recs_top5.parquet\", header=True, inferSchema=True\n",
    ")\n",
    "recs10 = spark.read.parquet(\n",
    "    \"../datasets/cf_item_recs_top10.parquet\", header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# ─── 3) Compute hits per user ───────────────────────────────────────────────────\n",
    "hits5  = recs5.join(\n",
    "    val,\n",
    "    (recs5.user_id == val.user_id) &\n",
    "    (recs5.candidate == val.true_item),\n",
    "    how=\"inner\"\n",
    ").select(recs5.user_id).distinct()\n",
    "\n",
    "hits10 = recs10.join(\n",
    "    val,\n",
    "    (recs10.user_id == val.user_id) &\n",
    "    (recs10.candidate == val.true_item),\n",
    "    how=\"inner\"\n",
    ").select(recs10.user_id).distinct()\n",
    "\n",
    "# ─── 4) Compute recall metrics ─────────────────────────────────────────────────\n",
    "num_users = val.select(\"user_id\").distinct().count()\n",
    "recall5   = hits5.count()  / num_users\n",
    "recall10  = hits10.count() / num_users\n",
    "\n",
    "print(f\"Recall@5:  {recall5:.4f}\")\n",
    "print(f\"Recall@10: {recall10:.4f}\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7cec5-4539-45ce-9eff-4ee97caedc59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c298f10-7ae9-40e2-a1bb-a22ed119054a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 23:58:37 WARN Utils: Your hostname, pysparkcf-ii58cents resolves to a loopback address: 127.0.0.1; using 10.0.0.4 instead (on interface eth0)\n",
      "25/04/30 23:58:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/30 23:58:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "      .builder\n",
    "      .master(\"local[*]\")                        # <— run locally on all cores\n",
    "      .appName(\"ItemItemCF\")\n",
    "      .config(\"spark.driver.memory\", \"24g\")\n",
    "      .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "      .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "      .config(\"spark.speculation\", \"false\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd3a7ff-1dd2-4153-8df0-0a17640507e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "# ─── 1) Load interactions (user↔item) ───────────────────────────────────────────\n",
    "# We'll read only user_id and item_id (no timestamps) to avoid the Parquet timestamp issue.\n",
    "clicks = spark.read.parquet(\"../datasets/train_clicks.parquet\") \\\n",
    "              .select(\"user_id\", F.col(\"click_article_id\").alias(\"item_id\")) \\\n",
    "              .distinct()\n",
    "\n",
    "# ─── 2) Compute item–item co-occurrence and cosine similarity ────────────────\n",
    "#  2a) For each (user, item) pair, collect the set of items they saw.\n",
    "#      Actually we already have one record per (user,item), so:\n",
    "userItems = clicks\n",
    "\n",
    "#  2b) Self-join on user_id to get all item–item pairs per user\n",
    "pairs = (\n",
    "  userItems.alias(\"a\")\n",
    "           .join(userItems.alias(\"b\"), on=\"user_id\")\n",
    "           .where(F.col(\"a.item_id\") < F.col(\"b.item_id\"))  # only one direction\n",
    "           .select(\n",
    "               F.col(\"a.item_id\").alias(\"item_i\"),\n",
    "               F.col(\"b.item_id\").alias(\"item_j\")\n",
    "           )\n",
    ")\n",
    "\n",
    "#  2c) Count co-occurrences and compute norms\n",
    "coCounts = (\n",
    "  pairs\n",
    "    .groupBy(\"item_i\", \"item_j\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "#  To get cosine sim, we need each item’s total occurrences:\n",
    "itemCounts = clicks.groupBy(\"item_id\").count().withColumnRenamed(\"count\", \"ni\")\n",
    "# Join to get n_i and n_j\n",
    "sims = (\n",
    "  coCounts\n",
    "    .join(itemCounts.withColumnRenamed(\"item_id\",\"item_i\"), on=\"item_i\")\n",
    "    .join(itemCounts.withColumnRenamed(\"item_id\",\"item_j\"), on=\"item_j\")\n",
    "    .withColumn(\"cosine_sim\", F.col(\"count\") / (F.sqrt(F.col(\"ni\")*F.col(\"nj\"))))\n",
    "    .select(\"item_i\",\"item_j\",\"cosine_sim\")\n",
    ")\n",
    "\n",
    "# ─── 3) For each item, grab top-50 neighbors ─────────────────────────────────\n",
    "window50 = Window.partitionBy(\"item_i\").orderBy(F.col(\"cosine_sim\").desc)\n",
    "neighbors = (\n",
    "  sims\n",
    "    .withColumn(\"rank\", F.row_number().over(window50))\n",
    "    .where(F.col(\"rank\") <= 50)\n",
    "    .select(\"item_i\",\"item_j\",\"cosine_sim\")\n",
    ")\n",
    "neighbors.write.mode(\"overwrite\").parquet(\"../datasets/item_neighbors.parquet\")\n",
    "\n",
    "# ─── 4) Scoring for validation users ─────────────────────────────────────────\n",
    "# Load neighbors and your val interactions\n",
    "neigh = spark.read.parquet(\"../datasets/item_neighbors.parquet\")\n",
    "val   = spark.read.parquet(\"../datasets/valid_clicks.parquet\") \\\n",
    "              .select(\"user_id\", F.col(\"click_article_id\").alias(\"true_item\"))\n",
    "\n",
    "# Build each user's candidate set by:\n",
    "#   – Exploding their history\n",
    "#   – Joining to neighbors\n",
    "#   – Summing scores per candidate\n",
    "recs = (\n",
    "  clicks.alias(\"his\")                               # user's history\n",
    "        .join(neigh, on=(clicks.item_id == neigh.item_i))\n",
    "        .groupBy(\"user_id\", \"item_j\")\n",
    "        .agg(F.sum(\"cosine_sim\").alias(\"score\"))\n",
    "        .withColumnRenamed(\"item_j\",\"candidate_item\")\n",
    ")\n",
    "\n",
    "# Remove already-seen:\n",
    "recs_filtered = (\n",
    "  recs.join(clicks, \n",
    "            (recs.user_id == clicks.user_id) & \n",
    "            (recs.candidate_item == clicks.item_id), \n",
    "            how=\"left_anti\")\n",
    ")\n",
    "\n",
    "# Rank and pick top-K\n",
    "windowK = Window.partitionBy(\"user_id\").orderBy(F.col(\"score\").desc)\n",
    "final = (\n",
    "  recs_filtered\n",
    "    .withColumn(\"rank\", F.row_number().over(windowK))\n",
    "    .where(F.col(\"rank\") <= 10)  # you can do <=5 for top-5 separate if you like\n",
    ")\n",
    "\n",
    "# ─── 5) Evaluate Recall@5/10 ────────────────────────────────────────────────\n",
    "# Bring in the ground truth and check for hits\n",
    "joined = final.join(val, \n",
    "                    (final.user_id == val.user_id) &\n",
    "                    (final.candidate_item == val.true_item),\n",
    "                    how=\"left\")\n",
    "totalUsers = val.select(\"user_id\").distinct().count()\n",
    "\n",
    "for K in (5,10):\n",
    "  hits = joined.where(F.col(\"rank\") <= K).filter(F.col(\"true_item\").isNotNull()).count()\n",
    "  print(f\"Recall@{K}: {hits/totalUsers:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863cd1c8-3bba-4e67-9bfc-033b0ea7b2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0811da-062c-412b-b178-772de5b7a1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa22fa1-07c2-4066-a0b6-04e4dc5499ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e130b3f-d44d-475b-90e9-8eaf3f0fec04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9d12a-1c6e-45ed-b8e9-dd506a902a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed59629-d991-4e6c-b751-3d4e1f1d07f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2ba86-1091-4649-8e70-61f886c9c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 0) Imports & start Spark with sane configs\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"ItemItemCF\")\n",
    "    # put scratch shuffle files on /mnt/data if available\n",
    "    .config(\"spark.local.dir\", \"/mnt/data/spark-local\")\n",
    "    # disable small broadcast joins\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "    # disable speculative (to avoid file‐delete races)\n",
    "    .config(\"spark.speculation\", \"false\")\n",
    "    # tune shuffle parallelism to about 200 partitions\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    # give the driver plenty of heap\n",
    "    .config(\"spark.driver.memory\", \"24g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Load minimal train + validation interactions\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# only pull user_id and click_article_id (drop timestamps entirely)\n",
    "train = (\n",
    "    spark.read\n",
    "         .parquet(\"../datasets/train_clicks.parquet\")\n",
    "         .select(\"user_id\", \"click_article_id\")\n",
    "         .withColumnRenamed(\"click_article_id\", \"item_id\")\n",
    "         .distinct()\n",
    ")\n",
    "val = (\n",
    "    spark.read\n",
    "         .parquet(\"../datasets/valid_clicks.parquet\")\n",
    "         .select(\"user_id\", \"click_article_id\")\n",
    "         .withColumnRenamed(\"click_article_id\", \"item_id\")\n",
    "         .distinct()\n",
    ")\n",
    "\n",
    "# build a map of held‐out items for recall calculation\n",
    "heldout = val.cache()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Compute item–item co-occurrence & similarity\n",
    "#    For scalability we only keep top-50 neighbors per item.\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# (a) join train to itself on user, filter item1 < item2 to avoid duplicates\n",
    "pairs = (\n",
    "    train.alias(\"A\")\n",
    "         .join(train.alias(\"B\"), on=\"user_id\")\n",
    "         .where(F.col(\"A.item_id\") < F.col(\"B.item_id\"))\n",
    "         .select(\n",
    "             F.col(\"A.item_id\").alias(\"i1\"),\n",
    "             F.col(\"B.item_id\").alias(\"i2\")\n",
    "         )\n",
    ")\n",
    "\n",
    "# (b) count co-occurrence\n",
    "co = (\n",
    "    pairs\n",
    "    .groupBy(\"i1\", \"i2\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# (c) get individual item support counts\n",
    "item_counts = (\n",
    "    train.groupBy(\"item_id\")\n",
    "         .count()\n",
    "         .withColumnRenamed(\"count\", \"ni\")\n",
    ")\n",
    "\n",
    "# (d) join counts & compute cosine‐style similarity:\n",
    "#     sim(i1,i2) = co_count / sqrt(count(i1)*count(i2))\n",
    "sim = (\n",
    "    co\n",
    "    .join(item_counts.withColumnRenamed(\"item_id\",\"i1\"), on=\"i1\")\n",
    "    .join(item_counts.withColumnRenamed(\"item_id\",\"i2\"), on=\"i2\")\n",
    "    .withColumn(\n",
    "        \"sim\",\n",
    "        F.col(\"count\") / F.sqrt(F.col(\"ni\") * F.col(\"nj\"))\n",
    "    )\n",
    "    .select(\"i1\",\"i2\",\"sim\")\n",
    ")\n",
    "\n",
    "# (e) get top-50 neighbors per i1 and per i2 (symmetrically)\n",
    "topN = (\n",
    "    sim\n",
    "    .unionByName(sim.select(F.col(\"i2\").alias(\"i1\"),\n",
    "                             F.col(\"i1\").alias(\"i2\"),\n",
    "                             \"sim\"))\n",
    "    # window to pick top 50 per i1\n",
    "    .withColumn(\n",
    "        \"rank\",\n",
    "        F.row_number().over(\n",
    "           Window.partitionBy(\"i1\")\n",
    "                 .orderBy(F.col(\"sim\").desc())\n",
    "        )\n",
    "    )\n",
    "    .where(F.col(\"rank\") <= 50)\n",
    "    .select(\"i1\",\"i2\",\"sim\")\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3) For each user, collect neighborhood scores from all items she’s seen\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# explode user’s train items → join to neighbors → aggregate score\n",
    "cand = (\n",
    "    train.alias(\"T\")\n",
    "         .join(topN.alias(\"N\"), F.col(\"T.item_id\")==F.col(\"N.i1\"))\n",
    "         .select(\n",
    "             \"user_id\",\n",
    "             F.col(\"i2\").alias(\"candidate\"),\n",
    "             \"sim\"\n",
    "         )\n",
    "         # sum up similarities from all seed items\n",
    "         .groupBy(\"user_id\",\"candidate\")\n",
    "         .agg(F.sum(\"sim\").alias(\"score\"))\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Filter out already‐seen and held-out items, pick top-K\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# (a) anti-join seen\n",
    "cand_filtered = (\n",
    "    cand.join(\n",
    "      train.withColumnRenamed(\"item_id\",\"candidate\"),\n",
    "      on=[\"user_id\",\"candidate\"], how=\"left_anti\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# (b) for Recall compute against held‐out, but to produce recs we also\n",
    "#     filter out any that are in heldout (so we’re not “predicting” them twice)\n",
    "cand_filtered = (\n",
    "    cand_filtered.join(\n",
    "      heldout.withColumnRenamed(\"item_id\",\"candidate\"),\n",
    "      on=[\"user_id\",\"candidate\"], how=\"left_anti\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# (c) pick top-5 & top-10 per user\n",
    "window = Window.partitionBy(\"user_id\").orderBy(F.col(\"score\").desc())\n",
    "recs = (\n",
    "    cand_filtered\n",
    "      .withColumn(\"rank\", F.row_number().over(window))\n",
    "      .filter(F.col(\"rank\") <= 10)\n",
    "      .select(\"user_id\",\"candidate\",\"score\",\"rank\")\n",
    "      .withColumnRenamed(\"candidate\",\"article_id\")\n",
    ")\n",
    "\n",
    "top5  = recs.filter(F.col(\"rank\") <= 5)\n",
    "top10 = recs.filter(F.col(\"rank\") <= 10)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 5) Persist results (Parquet, coalesced to ~50 files)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "top5 .coalesce(50).write.mode(\"overwrite\").parquet(\"../datasets/cf_item_recs_top5.parquet\")\n",
    "top10.coalesce(50).write.mode(\"overwrite\").parquet(\"../datasets/cf_item_recs_top10.parquet\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 6) Compute Recall@5 / Recall@10 on held-out\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# join recs to held-out → count hits per user → average\n",
    "for K in (5,10):\n",
    "    hits = (\n",
    "        recs.filter(F.col(\"rank\") <= K)\n",
    "            .join(heldout, [\"user_id\",\"article_id\"])\n",
    "            .groupBy(\"user_id\")\n",
    "            .count()\n",
    "            .withColumnRenamed(\"count\",\"hits\")\n",
    "    )\n",
    "    total_users = heldout.select(\"user_id\").distinct().count()\n",
    "    recall = hits.agg(F.avg(\"hits\")/1.0).collect()[0][0]  # since each user has exactly one held-out\n",
    "    print(f\"Recall@{K}: {recall:0.4f}\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d930e4-ab88-47a5-a539-8b203ae0f666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark-env)",
   "language": "python",
   "name": "spark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
