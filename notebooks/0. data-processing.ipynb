{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 1. Get dataset from the Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1746016529724
    }
   },
   "outputs": [],
   "source": [
    "# !wget \"https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/AI+Engineer/Project+9+-+Réalisez+une+application+mobile+de+recommandation+de+contenu/news-portal-user-interactions-by-globocom.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1745269495026
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/ocp8-cpu/code/Users/george.gvishiani/ocp9/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 2. Unzip all files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1745759756110
    }
   },
   "outputs": [],
   "source": [
    "# !unzip \"news-portal-user-interactions-by-globocom.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1745707335587
    }
   },
   "outputs": [],
   "source": [
    "# !unzip \"clicks.zip\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1745707339467
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "gather": {
     "logged": 1745780455853
    }
   },
   "outputs": [],
   "source": [
    "# clicks = pd.read_csv(\"clicks.csv\")\n",
    "clicks_sample = pd.read_csv(\"../datasets/clicks_sample.csv\")\n",
    "articles_metadata = pd.read_csv(\"../datasets/articles_metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2.1 Dataset: clicks_sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1745707370412
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1883, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1745707371616
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1883 entries, 0 to 1882\n",
      "Data columns (total 12 columns):\n",
      " #   Column               Non-Null Count  Dtype\n",
      "---  ------               --------------  -----\n",
      " 0   user_id              1883 non-null   int64\n",
      " 1   session_id           1883 non-null   int64\n",
      " 2   session_start        1883 non-null   int64\n",
      " 3   session_size         1883 non-null   int64\n",
      " 4   click_article_id     1883 non-null   int64\n",
      " 5   click_timestamp      1883 non-null   int64\n",
      " 6   click_environment    1883 non-null   int64\n",
      " 7   click_deviceGroup    1883 non-null   int64\n",
      " 8   click_os             1883 non-null   int64\n",
      " 9   click_country        1883 non-null   int64\n",
      " 10  click_region         1883 non-null   int64\n",
      " 11  click_referrer_type  1883 non-null   int64\n",
      "dtypes: int64(12)\n",
      "memory usage: 176.7 KB\n"
     ]
    }
   ],
   "source": [
    "clicks_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1745707375475
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>session_start</th>\n",
       "      <th>session_size</th>\n",
       "      <th>click_article_id</th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>click_environment</th>\n",
       "      <th>click_deviceGroup</th>\n",
       "      <th>click_os</th>\n",
       "      <th>click_country</th>\n",
       "      <th>click_region</th>\n",
       "      <th>click_referrer_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1506825423271737</td>\n",
       "      <td>1506825423000</td>\n",
       "      <td>2</td>\n",
       "      <td>157541</td>\n",
       "      <td>1506826828020</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1506825423271737</td>\n",
       "      <td>1506825423000</td>\n",
       "      <td>2</td>\n",
       "      <td>68866</td>\n",
       "      <td>1506826858020</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1506825426267738</td>\n",
       "      <td>1506825426000</td>\n",
       "      <td>2</td>\n",
       "      <td>235840</td>\n",
       "      <td>1506827017951</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1506825426267738</td>\n",
       "      <td>1506825426000</td>\n",
       "      <td>2</td>\n",
       "      <td>96663</td>\n",
       "      <td>1506827047951</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1506825435299739</td>\n",
       "      <td>1506825435000</td>\n",
       "      <td>2</td>\n",
       "      <td>119592</td>\n",
       "      <td>1506827090575</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id        session_id  session_start  session_size  click_article_id  \\\n",
       "0        0  1506825423271737  1506825423000             2            157541   \n",
       "1        0  1506825423271737  1506825423000             2             68866   \n",
       "2        1  1506825426267738  1506825426000             2            235840   \n",
       "3        1  1506825426267738  1506825426000             2             96663   \n",
       "4        2  1506825435299739  1506825435000             2            119592   \n",
       "\n",
       "   click_timestamp  click_environment  click_deviceGroup  click_os  \\\n",
       "0    1506826828020                  4                  3        20   \n",
       "1    1506826858020                  4                  3        20   \n",
       "2    1506827017951                  4                  1        17   \n",
       "3    1506827047951                  4                  1        17   \n",
       "4    1506827090575                  4                  1        17   \n",
       "\n",
       "   click_country  click_region  click_referrer_type  \n",
       "0              1            20                    2  \n",
       "1              1            20                    2  \n",
       "2              1            16                    2  \n",
       "3              1            16                    2  \n",
       "4              1            24                    2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "gather": {
     "logged": 1745708235841
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                 707\n",
       "session_id              707\n",
       "session_start           619\n",
       "session_size             11\n",
       "click_article_id        323\n",
       "click_timestamp        1883\n",
       "click_environment         3\n",
       "click_deviceGroup         3\n",
       "click_os                  6\n",
       "click_country             7\n",
       "click_region             26\n",
       "click_referrer_type       6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks_sample.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1745707377659
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>session_start</th>\n",
       "      <th>session_size</th>\n",
       "      <th>click_article_id</th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>click_environment</th>\n",
       "      <th>click_deviceGroup</th>\n",
       "      <th>click_os</th>\n",
       "      <th>click_country</th>\n",
       "      <th>click_region</th>\n",
       "      <th>click_referrer_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1883.000000</td>\n",
       "      <td>1.883000e+03</td>\n",
       "      <td>1.883000e+03</td>\n",
       "      <td>1883.000000</td>\n",
       "      <td>1883.000000</td>\n",
       "      <td>1.883000e+03</td>\n",
       "      <td>1883.000000</td>\n",
       "      <td>1883.000000</td>\n",
       "      <td>1883.000000</td>\n",
       "      <td>1883.000000</td>\n",
       "      <td>1883.000000</td>\n",
       "      <td>1883.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>355.893787</td>\n",
       "      <td>1.506828e+15</td>\n",
       "      <td>1.506828e+12</td>\n",
       "      <td>3.459904</td>\n",
       "      <td>176717.848646</td>\n",
       "      <td>1.506830e+12</td>\n",
       "      <td>3.917153</td>\n",
       "      <td>2.305895</td>\n",
       "      <td>12.113648</td>\n",
       "      <td>1.491768</td>\n",
       "      <td>18.774827</td>\n",
       "      <td>1.764206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>206.162865</td>\n",
       "      <td>8.679628e+08</td>\n",
       "      <td>8.679468e+05</td>\n",
       "      <td>3.037467</td>\n",
       "      <td>82324.177259</td>\n",
       "      <td>1.064938e+07</td>\n",
       "      <td>0.410461</td>\n",
       "      <td>1.062301</td>\n",
       "      <td>7.825735</td>\n",
       "      <td>2.007772</td>\n",
       "      <td>7.083400</td>\n",
       "      <td>1.225679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.506825e+15</td>\n",
       "      <td>1.506825e+12</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2137.000000</td>\n",
       "      <td>1.506827e+12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>181.500000</td>\n",
       "      <td>1.506827e+15</td>\n",
       "      <td>1.506827e+12</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>108854.000000</td>\n",
       "      <td>1.506828e+12</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>353.000000</td>\n",
       "      <td>1.506828e+15</td>\n",
       "      <td>1.506828e+12</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>157541.000000</td>\n",
       "      <td>1.506828e+12</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>537.000000</td>\n",
       "      <td>1.506828e+15</td>\n",
       "      <td>1.506828e+12</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>236697.500000</td>\n",
       "      <td>1.506829e+12</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>706.000000</td>\n",
       "      <td>1.506829e+15</td>\n",
       "      <td>1.506829e+12</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>363291.000000</td>\n",
       "      <td>1.506998e+12</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           user_id    session_id  session_start  session_size  \\\n",
       "count  1883.000000  1.883000e+03   1.883000e+03   1883.000000   \n",
       "mean    355.893787  1.506828e+15   1.506828e+12      3.459904   \n",
       "std     206.162865  8.679628e+08   8.679468e+05      3.037467   \n",
       "min       0.000000  1.506825e+15   1.506825e+12      2.000000   \n",
       "25%     181.500000  1.506827e+15   1.506827e+12      2.000000   \n",
       "50%     353.000000  1.506828e+15   1.506828e+12      3.000000   \n",
       "75%     537.000000  1.506828e+15   1.506828e+12      4.000000   \n",
       "max     706.000000  1.506829e+15   1.506829e+12     24.000000   \n",
       "\n",
       "       click_article_id  click_timestamp  click_environment  \\\n",
       "count       1883.000000     1.883000e+03        1883.000000   \n",
       "mean      176717.848646     1.506830e+12           3.917153   \n",
       "std        82324.177259     1.064938e+07           0.410461   \n",
       "min         2137.000000     1.506827e+12           1.000000   \n",
       "25%       108854.000000     1.506828e+12           4.000000   \n",
       "50%       157541.000000     1.506828e+12           4.000000   \n",
       "75%       236697.500000     1.506829e+12           4.000000   \n",
       "max       363291.000000     1.506998e+12           4.000000   \n",
       "\n",
       "       click_deviceGroup     click_os  click_country  click_region  \\\n",
       "count        1883.000000  1883.000000    1883.000000   1883.000000   \n",
       "mean            2.305895    12.113648       1.491768     18.774827   \n",
       "std             1.062301     7.825735       2.007772      7.083400   \n",
       "min             1.000000     2.000000       1.000000      1.000000   \n",
       "25%             1.000000     2.000000       1.000000     13.000000   \n",
       "50%             3.000000    17.000000       1.000000     21.000000   \n",
       "75%             3.000000    19.000000       1.000000     25.000000   \n",
       "max             4.000000    20.000000      11.000000     28.000000   \n",
       "\n",
       "       click_referrer_type  \n",
       "count          1883.000000  \n",
       "mean              1.764206  \n",
       "std               1.225679  \n",
       "min               1.000000  \n",
       "25%               1.000000  \n",
       "50%               1.000000  \n",
       "75%               2.000000  \n",
       "max               7.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks_sample.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2.2 Dataset: articles_metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "gather": {
     "logged": 1745780463321
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364047, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gather": {
     "logged": 1745269692022
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 364047 entries, 0 to 364046\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count   Dtype\n",
      "---  ------         --------------   -----\n",
      " 0   article_id     364047 non-null  int64\n",
      " 1   category_id    364047 non-null  int64\n",
      " 2   created_at_ts  364047 non-null  int64\n",
      " 3   publisher_id   364047 non-null  int64\n",
      " 4   words_count    364047 non-null  int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 13.9 MB\n"
     ]
    }
   ],
   "source": [
    "articles_metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gather": {
     "logged": 1745269693410
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>created_at_ts</th>\n",
       "      <th>publisher_id</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1513144419000</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1405341936000</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1408667706000</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1408468313000</td>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1407071171000</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  category_id  created_at_ts  publisher_id  words_count\n",
       "0           0            0  1513144419000             0          168\n",
       "1           1            1  1405341936000             0          189\n",
       "2           2            1  1408667706000             0          250\n",
       "3           3            1  1408468313000             0          230\n",
       "4           4            1  1407071171000             0          162"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "gather": {
     "logged": 1745269696350
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>created_at_ts</th>\n",
       "      <th>publisher_id</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>364047.000000</td>\n",
       "      <td>364047.000000</td>\n",
       "      <td>3.640470e+05</td>\n",
       "      <td>364047.0</td>\n",
       "      <td>364047.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>182023.000000</td>\n",
       "      <td>283.108239</td>\n",
       "      <td>1.474070e+12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.897727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>105091.461061</td>\n",
       "      <td>136.723470</td>\n",
       "      <td>4.293038e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.502766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.159356e+12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>91011.500000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>1.444925e+12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>182023.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>1.489422e+12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>186.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>273034.500000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>1.509891e+12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>218.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>364046.000000</td>\n",
       "      <td>460.000000</td>\n",
       "      <td>1.520943e+12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6690.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          article_id    category_id  created_at_ts  publisher_id  \\\n",
       "count  364047.000000  364047.000000   3.640470e+05      364047.0   \n",
       "mean   182023.000000     283.108239   1.474070e+12           0.0   \n",
       "std    105091.461061     136.723470   4.293038e+10           0.0   \n",
       "min         0.000000       0.000000   1.159356e+12           0.0   \n",
       "25%     91011.500000     199.000000   1.444925e+12           0.0   \n",
       "50%    182023.000000     301.000000   1.489422e+12           0.0   \n",
       "75%    273034.500000     399.000000   1.509891e+12           0.0   \n",
       "max    364046.000000     460.000000   1.520943e+12           0.0   \n",
       "\n",
       "         words_count  \n",
       "count  364047.000000  \n",
       "mean      190.897727  \n",
       "std        59.502766  \n",
       "min         0.000000  \n",
       "25%       159.000000  \n",
       "50%       186.000000  \n",
       "75%       218.000000  \n",
       "max      6690.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_metadata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2.3 Many files on clicks per hour in clicks.zip file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1745694566380
    }
   },
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "# from azureml.core import Workspace, Datastore, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "gather": {
     "logged": 1745708678480
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: 1506826800026\n",
      "max: 1506998154157\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, glob, os\n",
    "\n",
    "# grab a sample of raw values\n",
    "paths = glob.glob(os.path.join(\"../datasets/clicks\", \"*.csv\"))\n",
    "sample = pd.read_csv(paths[0], usecols=[\"click_timestamp\"])[\"click_timestamp\"]\n",
    "\n",
    "print(\"min:\", sample.min())\n",
    "print(\"max:\", sample.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "gather": {
     "logged": 1745709095476
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 385 files under '../datasets/clicks/'\n",
      "user_id                        uint16\n",
      "session_id                     uint16\n",
      "session_start          datetime64[ns]\n",
      "session_size                    uint8\n",
      "click_article_id               uint16\n",
      "click_timestamp        datetime64[ns]\n",
      "click_environment               uint8\n",
      "click_deviceGroup               uint8\n",
      "click_os                        uint8\n",
      "click_country                   uint8\n",
      "click_region                    uint8\n",
      "click_referrer_type             uint8\n",
      "dtype: object\n",
      "Combined DataFrame memory usage: 82.64 MB\n",
      "Wrote combined DataFrame to 'combined_clicks.parquet'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Define the smallest-possible dtypes for each integer column\n",
    "clicks_dtypes = {\n",
    "    \"user_id\":             \"uint16\",\n",
    "    \"session_id\":          \"uint16\",\n",
    "    \"session_size\":        \"uint8\",\n",
    "    \"click_article_id\":    \"uint16\",\n",
    "    \"click_environment\":   \"uint8\",\n",
    "    \"click_deviceGroup\":   \"uint8\",\n",
    "    \"click_os\":            \"uint8\",\n",
    "    \"click_country\":       \"uint8\",\n",
    "    \"click_region\":        \"uint8\",\n",
    "    \"click_referrer_type\": \"uint8\"\n",
    "}\n",
    "\n",
    "# 2. Find all CSV files in the clicks directory\n",
    "clicks_dir = \"../datasets/clicks\"\n",
    "csv_files = glob.glob(os.path.join(clicks_dir, \"*.csv\"))\n",
    "print(f\"Found {len(csv_files)} files under '{clicks_dir}/'\")\n",
    "\n",
    "# 3. Read each CSV with enforced dtypes, convert timestamps, collect into a list\n",
    "dfs = []\n",
    "for fp in csv_files:\n",
    "    df = pd.read_csv(fp, dtype=clicks_dtypes)\n",
    "    # Convert epoch-milliseconds → datetime64[ns]\n",
    "    df[\"session_start\"]   = pd.to_datetime(df[\"session_start\"],   unit=\"ms\")\n",
    "    df[\"click_timestamp\"] = pd.to_datetime(df[\"click_timestamp\"], unit=\"ms\")\n",
    "    dfs.append(df)\n",
    "\n",
    "# 4. Concatenate into one DataFrame\n",
    "clicks_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 5. Inspect dtypes and memory footprint\n",
    "print(clicks_df.dtypes)\n",
    "mem_mb = clicks_df.memory_usage(deep=True).sum() / 2**20\n",
    "print(f\"Combined DataFrame memory usage: {mem_mb:.2f} MB\")\n",
    "\n",
    "# 6. (Optional) Persist to disk for downstream use\n",
    "output_path = \"../datasets/combined_clicks.parquet\"\n",
    "clicks_df.to_parquet(output_path, index=False)\n",
    "print(f\"Wrote combined DataFrame to '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "gather": {
     "logged": 1745709394059
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote combined DataFrame to '../datasets/combined_clicks.parquet'\n"
     ]
    }
   ],
   "source": [
    "# 6. (Optional) Persist to disk for downstream use\n",
    "output_path = \"../datasets/combined_clicks.parquet\"\n",
    "clicks_df.to_parquet(output_path, index=False)\n",
    "print(f\"Wrote combined DataFrame to '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1745760095803
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988181, 12)\n",
      "user_id                        uint16\n",
      "session_id                     uint16\n",
      "session_start          datetime64[ns]\n",
      "session_size                    uint8\n",
      "click_article_id               uint16\n",
      "click_timestamp        datetime64[ns]\n",
      "click_environment               uint8\n",
      "click_deviceGroup               uint8\n",
      "click_os                        uint8\n",
      "click_country                   uint8\n",
      "click_region                    uint8\n",
      "click_referrer_type             uint8\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the combined clicks Parquet\n",
    "clicks_df = pd.read_parquet(\"../datasets/combined_clicks.parquet\")\n",
    "\n",
    "# Quick sanity check\n",
    "print(clicks_df.shape)\n",
    "print(clicks_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gather": {
     "logged": 1745740190738
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click timestamps span from 2017-10-01 02:37:03 to 2017-10-17 03:36:19\n",
      "Date range: 2017-10-01 → 2017-10-17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Explicit min/max for clear range\n",
    "min_ts = clicks_df['session_start'].min()\n",
    "max_ts = clicks_df['session_start'].max()\n",
    "print(f\"Click timestamps span from {min_ts} to {max_ts}\")\n",
    "print(f\"Date range: {min_ts.date()} → {max_ts.date()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2.4 Create Train/Validation/Testing Dataset - Last Click Out for Each User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1745760165308
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load & sort\n",
    "clicks_df = pd.read_parquet(\"../datasets/combined_clicks.parquet\")\n",
    "clicks_df = clicks_df.sort_values([\"user_id\", \"click_timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1745760212560
    }
   },
   "outputs": [],
   "source": [
    "# 2. Identify test clicks (last per user)\n",
    "last_idx = clicks_df.groupby(\"user_id\")[\"click_timestamp\"].idxmax()\n",
    "is_test  = clicks_df.index.isin(last_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1745760282228
    }
   },
   "outputs": [],
   "source": [
    "# 3. From the TRAIN portion, identify validation clicks (now the last in train_df)\n",
    "train_df_full = clicks_df.loc[~is_test]\n",
    "val_idx       = train_df_full.groupby(\"user_id\")[\"click_timestamp\"].idxmax()\n",
    "is_val        = clicks_df.index.isin(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1745760301743
    }
   },
   "outputs": [],
   "source": [
    "# 4. Build splits\n",
    "train_df = clicks_df.loc[~is_test & ~is_val].reset_index(drop=True)\n",
    "val_df   = clicks_df.loc[ is_val         ].reset_index(drop=True)\n",
    "test_df  = clicks_df.loc[ is_test        ].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1745760367058
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2,857,109 clicks\n",
      "Val:   65,536 clicks (one per user)\n",
      "Test:  65,536 clicks (one per user)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {len(train_df):,} clicks\")\n",
    "print(f\"Val:   {len(val_df):,} clicks (one per user)\")\n",
    "print(f\"Test:  {len(test_df):,} clicks (one per user)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1745760585333
    }
   },
   "outputs": [],
   "source": [
    "output_path_train = \"../datasets/train_clicks.parquet\"\n",
    "output_path_valid = \"../datasets/valid_clicks.parquet\"\n",
    "output_path_test = \"../datasets/test_clicks.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1745760664907
    }
   },
   "outputs": [],
   "source": [
    "train_df.to_parquet(output_path_train, index=False)\n",
    "val_df.to_parquet(output_path_valid, index=False)\n",
    "test_df.to_parquet(output_path_test, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gather": {
     "logged": 1745761833630
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "# ── CONFIG ────────────────────────────────────────────────────\n",
    "TRAIN_PATH        = \"../datasets/train_clicks.parquet\"\n",
    "VAL_PATH          = \"../datasets/valid_clicks.parquet\"\n",
    "TEST_PATH         = \"../datasets/test_clicks.parquet\"\n",
    "META_PATH         = \"../datasets/articles_metadata.csv\"   # or your parquet copy\n",
    "HALF_LIFE_DAYS    = [1, 3, 7, 14]\n",
    "BETA_VALUES       = [0.0, 0.05, 0.1, 0.2]\n",
    "FRESH_WINDOW_DAYS = 1\n",
    "TOP_M             = 500\n",
    "RECALL_K          = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gather": {
     "logged": 1745761795574
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (2857109, 12), Val: (65536, 12), Test: (65536, 12)\n"
     ]
    }
   ],
   "source": [
    "# 1) Load train / val / test\n",
    "train_df = pd.read_parquet(TRAIN_PATH)\n",
    "val_df   = pd.read_parquet(VAL_PATH)\n",
    "test_df  = pd.read_parquet(TEST_PATH)\n",
    "\n",
    "# ensure timestamps are datetime\n",
    "for df in (train_df, val_df, test_df):\n",
    "    df[\"click_timestamp\"] = pd.to_datetime(df[\"click_timestamp\"], unit=\"ms\")\n",
    "\n",
    "print(f\"Train: {train_df.shape}, Val: {val_df.shape}, Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "gather": {
     "logged": 1745761840099
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364047, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>created_at_ts</th>\n",
       "      <th>publisher_id</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-13 05:53:39</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-07-14 12:45:36</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-08-22 00:35:06</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-08-19 17:11:53</td>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-08-03 13:06:11</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  category_id       created_at_ts  publisher_id  words_count\n",
       "0           0            0 2017-12-13 05:53:39             0          168\n",
       "1           1            1 2014-07-14 12:45:36             0          189\n",
       "2           2            1 2014-08-22 00:35:06             0          250\n",
       "3           3            1 2014-08-19 17:11:53             0          230\n",
       "4           4            1 2014-08-03 13:06:11             0          162"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Load metadata\n",
    "dtypes = {\n",
    "        \"article_id\": \"uint32\",\n",
    "        \"category_id\": \"uint16\",\n",
    "        \"publisher_id\": \"uint8\",\n",
    "        \"words_count\": \"uint16\"\n",
    "    }\n",
    "\n",
    "articles = pd.read_csv(META_PATH,dtype=dtypes)\n",
    "\n",
    "articles[\"created_at_ts\"] = pd.to_datetime(articles[\"created_at_ts\"], unit=\"ms\")\n",
    "print(articles.shape)\n",
    "articles.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "gather": {
     "logged": 1745762736320
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_scores(train_df, articles, half_life_days, beta, fresh_window_days, top_m=500):\n",
    "    # 1) global reference time\n",
    "    now = train_df[\"click_timestamp\"].max()\n",
    "    # 2) decay constant (per second)\n",
    "    λ = np.log(2) / pd.Timedelta(days=half_life_days).total_seconds()\n",
    "    # 3) per-click weights\n",
    "    ages    = (now - train_df[\"click_timestamp\"]).dt.total_seconds()\n",
    "    weights = np.exp(-λ * ages)\n",
    "    # 4) aggregate per article\n",
    "    pop = (\n",
    "        train_df.assign(weight=weights)\n",
    "                .groupby(\"click_article_id\")[\"weight\"]\n",
    "                .sum()\n",
    "                .rename(\"pop_score\")\n",
    "                .to_frame()\n",
    "    )\n",
    "    # 5) join publication times\n",
    "    pop = pop.join(\n",
    "        articles.set_index(\"article_id\")[[\"created_at_ts\"]],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    # 6) apply freshness boost\n",
    "    is_fresh = (now - pop[\"created_at_ts\"]) <= pd.Timedelta(days=fresh_window_days)\n",
    "    pop[\"final_score\"] = pop[\"pop_score\"] * (1 + beta * is_fresh.astype(float))\n",
    "    # 7) build cleaned candidates DF\n",
    "    top_series = pop[\"final_score\"].nlargest(top_m)\n",
    "    cands = pd.DataFrame({\n",
    "        \"article_id\": top_series.index.astype(int),\n",
    "        \"final_score\": top_series.values\n",
    "    })\n",
    "    return cands\n",
    "\n",
    "\n",
    "def recall_at_k(cands, articles, holdout_df, K):\n",
    "    # pull publication dates\n",
    "    pub_dates = articles.set_index(\"article_id\")[\"created_at_ts\"].to_dict()\n",
    "    assert \"article_id\" in cands.columns, \"cands missing article_id\"\n",
    "    \n",
    "    # user → cutoff map\n",
    "    user_cutoff = holdout_df.set_index(\"user_id\")[\"click_timestamp\"].to_dict()\n",
    "    art_ids = cands[\"article_id\"].tolist()\n",
    "    hits = []\n",
    "    for _, row in holdout_df.iterrows():\n",
    "        uid, true_a = row[\"user_id\"], row[\"click_article_id\"]\n",
    "        cutoff = user_cutoff[uid]\n",
    "        # keep only articles published ≤ cutoff\n",
    "        valid = [a for a in art_ids if pub_dates.get(a, pd.Timestamp(0)) <= cutoff]\n",
    "        topk  = valid[:K]\n",
    "        hits.append(true_a in topk)\n",
    "    return np.mean(hits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "gather": {
     "logged": 1745763798276
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry 1/16: h=1, β=0.0\n",
      " → Recall@10: 0.0714\n",
      "\n",
      "Entry 2/16: h=1, β=0.05\n",
      " → Recall@10: 0.0714\n",
      "\n",
      "Entry 3/16: h=1, β=0.1\n",
      " → Recall@10: 0.0714\n",
      "\n",
      "Entry 4/16: h=1, β=0.2\n",
      " → Recall@10: 0.0714\n",
      "\n",
      "Entry 5/16: h=3, β=0.0\n",
      " → Recall@10: 0.1470\n",
      "\n",
      "Entry 6/16: h=3, β=0.05\n",
      " → Recall@10: 0.1470\n",
      "\n",
      "Entry 7/16: h=3, β=0.1\n",
      " → Recall@10: 0.1470\n",
      "\n",
      "Entry 8/16: h=3, β=0.2\n",
      " → Recall@10: 0.1470\n",
      "\n",
      "Entry 9/16: h=7, β=0.0\n",
      " → Recall@10: 0.0582\n",
      "\n",
      "Entry 10/16: h=7, β=0.05\n",
      " → Recall@10: 0.0582\n",
      "\n",
      "Entry 11/16: h=7, β=0.1\n",
      " → Recall@10: 0.0582\n",
      "\n",
      "Entry 12/16: h=7, β=0.2\n",
      " → Recall@10: 0.0582\n",
      "\n",
      "Entry 13/16: h=14, β=0.0\n",
      " → Recall@10: 0.0444\n",
      "\n",
      "Entry 14/16: h=14, β=0.05\n",
      " → Recall@10: 0.0444\n",
      "\n",
      "Entry 15/16: h=14, β=0.1\n",
      " → Recall@10: 0.0444\n",
      "\n",
      "Entry 16/16: h=14, β=0.2\n",
      " → Recall@10: 0.0444\n",
      "\n",
      "Best hyperparameters:\n",
      " half_life_days    3.000000\n",
      "beta              0.000000\n",
      "recall@10         0.147034\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "results = []\n",
    "for counter, (h, beta) in enumerate(product(HALF_LIFE_DAYS, BETA_VALUES)):\n",
    "    print(f\"Entry {counter+1}/{len(HALF_LIFE_DAYS)*len(BETA_VALUES)}: h={h}, β={beta}\")\n",
    "    cands = compute_scores(train_df, articles, h, beta, FRESH_WINDOW_DAYS, top_m=TOP_M)\n",
    "    r = recall_at_k(cands, articles, val_df, RECALL_K)\n",
    "    results.append({\n",
    "        \"half_life_days\": h,\n",
    "        \"beta\": beta,\n",
    "        f\"recall@{RECALL_K}\": r\n",
    "    })\n",
    "    print(f\" → Recall@{RECALL_K}: {r:.4f}\\n\")\n",
    "\n",
    "df_res = pd.DataFrame(results)\n",
    "best = df_res.sort_values(f\"recall@{RECALL_K}\", ascending=False).iloc[0]\n",
    "print(\"Best hyperparameters:\\n\", best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3.2 Combine training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "gather": {
     "logged": 1745764109781
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cell: Final Evaluation on Test Set\n",
    "\n",
    "# 1) Load splits (if not already in memory)\n",
    "train_df = pd.read_parquet(\"../datasets/train_clicks.parquet\")\n",
    "val_df   = pd.read_parquet(\"../datasets/valid_clicks.parquet\")\n",
    "test_df  = pd.read_parquet(\"../datasets/test_clicks.parquet\")\n",
    "\n",
    "# Ensure timestamps are datetime\n",
    "for df in (train_df, val_df, test_df):\n",
    "    df[\"click_timestamp\"] = pd.to_datetime(df[\"click_timestamp\"], unit=\"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "gather": {
     "logged": 1745764140699
    }
   },
   "outputs": [],
   "source": [
    "# 2) Load metadata (if not already in memory)\n",
    "articles = pd.read_csv(\n",
    "    \"../datasets/articles_metadata.csv\",\n",
    "    dtype={\n",
    "        \"article_id\": \"uint32\",\n",
    "        \"category_id\": \"uint16\",\n",
    "        \"publisher_id\": \"uint8\",\n",
    "        \"words_count\": \"uint16\"\n",
    "    }\n",
    ")\n",
    "articles[\"created_at_ts\"] = pd.to_datetime(articles[\"created_at_ts\"], unit=\"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "gather": {
     "logged": 1745764160505
    }
   },
   "outputs": [],
   "source": [
    "# 3) Combine train + validation\n",
    "train_plus_val = pd.concat([train_df, val_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "gather": {
     "logged": 1745764172579
    }
   },
   "outputs": [],
   "source": [
    "# 4) Compute scores with best hyperparameters (h=3 days, β=0)\n",
    "cands = compute_scores(\n",
    "    train_plus_val,\n",
    "    articles,\n",
    "    half_life_days=3,\n",
    "    beta=0.0,\n",
    "    fresh_window_days=FRESH_WINDOW_DAYS,\n",
    "    top_m=TOP_M\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "gather": {
     "logged": 1745764239026
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Recall@10 (h=3, β=0): 0.1211\n"
     ]
    }
   ],
   "source": [
    "# 5) Evaluate on test set\n",
    "test_recall = recall_at_k(cands, articles, test_df, RECALL_K)\n",
    "print(f\"Final Test Recall@{RECALL_K} (h=3, β=0): {test_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 14:42:50 WARN Utils: Your hostname, p962cnts8crs64g128g resolves to a loopback address: 127.0.0.1; using 10.0.0.4 instead (on interface eth0)\n",
      "25/04/30 14:42:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/30 14:42:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, LongType, IntegerType, DoubleType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# ─── 1) Start Spark with console‐progress enabled ────────────────────────────────\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ContentBasedSpark\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# ─── 2) Config & broadcast data ─────────────────────────────────────────────────\n",
    "M_CANDIDATES      = 50\n",
    "K1, K2            = 5, 10\n",
    "EMB_FP            = \"../datasets/articles_embeddings.pickle\"\n",
    "META_FP           = \"../datasets/articles_metadata.csv\"\n",
    "TRAIN_FP          = \"../datasets/train_clicks.parquet\"\n",
    "VALID_FP          = \"../datasets/valid_clicks.parquet\"\n",
    "UP_FP             = \"../datasets/user_profiles.parquet\"\n",
    "\n",
    "# Load embeddings & normalize\n",
    "with open(EMB_FP, \"rb\") as f:\n",
    "    embs = pickle.load(f)\n",
    "emb_norm = embs / np.linalg.norm(embs, axis=1)[:, None]\n",
    "\n",
    "# Load metadata and build maps\n",
    "meta = pd.read_csv(META_FP, usecols=[\"article_id\",\"created_at_ts\"])\n",
    "meta[\"created_at_ts\"] = pd.to_datetime(meta[\"created_at_ts\"], unit=\"ms\")\n",
    "meta = meta.sort_values(\"article_id\").reset_index(drop=True)\n",
    "article_ids = meta[\"article_id\"].to_numpy()\n",
    "pub_map     = dict(zip(meta[\"article_id\"], meta[\"created_at_ts\"]))\n",
    "\n",
    "# Load clicks and build seen/val–time maps\n",
    "train_pd     = pd.read_parquet(TRAIN_FP, engine=\"pyarrow\")\n",
    "val_pd       = pd.read_parquet(VALID_FP, engine=\"pyarrow\")\n",
    "val_pd[\"click_timestamp\"] = pd.to_datetime(val_pd[\"click_timestamp\"], unit=\"ms\")\n",
    "seen_map     = train_pd.groupby(\"user_id\")[\"click_article_id\"].apply(set).to_dict()\n",
    "val_time_map = val_pd.set_index(\"user_id\")[\"click_timestamp\"].to_dict()\n",
    "\n",
    "# Broadcast to executors\n",
    "b_article_ids  = sc.broadcast(article_ids)\n",
    "b_emb_norm     = sc.broadcast(emb_norm)\n",
    "b_pub_map      = sc.broadcast(pub_map)\n",
    "b_seen_map     = sc.broadcast(seen_map)\n",
    "b_val_time_map = sc.broadcast(val_time_map)\n",
    "\n",
    "# Load user profiles\n",
    "up_df = spark.read.parquet(UP_FP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, IntegerType, DoubleType\n",
    "\n",
    "# Define output schema\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\",    LongType(),    False),\n",
    "    StructField(\"rank\",       IntegerType(), False),\n",
    "    StructField(\"article_id\", LongType(),    False),\n",
    "    StructField(\"score\",      DoubleType(),  False),\n",
    "])\n",
    "\n",
    "def recommend_batch(pdf_iter):\n",
    "    for pdf in pdf_iter:\n",
    "        recs = {\"user_id\":[], \"rank\":[], \"article_id\":[], \"score\":[]}\n",
    "        emb_cols = [c for c in pdf.columns if c != \"user_id\"]\n",
    "        for row in pdf.itertuples(index=False):\n",
    "            uid = int(row.user_id)\n",
    "            uemb = np.array([getattr(row,c) for c in emb_cols], dtype=float)\n",
    "            norm = np.linalg.norm(uemb)\n",
    "            u_norm = uemb/norm if norm>0 else uemb\n",
    "\n",
    "            sims = b_emb_norm.value.dot(u_norm)\n",
    "            idxs = np.argpartition(-sims, M_CANDIDATES)[:M_CANDIDATES]\n",
    "            idxs = idxs[np.argsort(-sims[idxs])]\n",
    "            aids = b_article_ids.value[idxs]\n",
    "            ss  = sims[idxs]\n",
    "\n",
    "            cutoff = b_val_time_map.value.get(uid, pd.Timestamp.max)\n",
    "            valid = [(a,s) for a,s in zip(aids,ss)\n",
    "                     if b_pub_map.value.get(a,pd.Timestamp.min) <= cutoff]\n",
    "\n",
    "            seen = b_seen_map.value.get(uid, set())\n",
    "            filtered = [(a,s) for a,s in valid if a not in seen]\n",
    "\n",
    "            for rank, (a,s) in enumerate(filtered[:K1], start=1):\n",
    "                recs[\"user_id\"].append(uid)\n",
    "                recs[\"rank\"].append(rank)\n",
    "                recs[\"article_id\"].append(int(a))\n",
    "                recs[\"score\"].append(float(s))\n",
    "            for rank, (a,s) in enumerate(filtered[:K2], start=1):\n",
    "                recs[\"user_id\"].append(uid)\n",
    "                recs[\"rank\"].append(rank)\n",
    "                recs[\"article_id\"].append(int(a))\n",
    "                recs[\"score\"].append(float(s))\n",
    "        \n",
    "        yield pd.DataFrame(recs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 14:44:05 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 1:===================================================>       (7 + 1) / 8]\r"
     ]
    }
   ],
   "source": [
    "# Apply the UDF in parallel\n",
    "result = up_df.mapInPandas(recommend_batch, schema=schema)\n",
    "\n",
    "# Write out your top-5 and top-10\n",
    "result.filter(\"rank <= 5\") \\\n",
    "      .write.mode(\"overwrite\") \\\n",
    "      .csv(\"../datasets/content_recs_top5\", header=True)\n",
    "\n",
    "result.filter(\"rank <= 10\") \\\n",
    "      .write.mode(\"overwrite\") \\\n",
    "      .csv(\"../datasets/content_recs_top10\", header=True)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "gather": {
     "logged": 1745956016939
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "(250,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3668it [19:44,  3.10it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m p_norm \u001b[38;5;241m=\u001b[39m uemb \u001b[38;5;241m/\u001b[39m norm \u001b[38;5;28;01mif\u001b[39;00m norm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m uemb\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# b) Cosine similarities\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m sims \u001b[38;5;241m=\u001b[39m \u001b[43memb_norm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# c) Top-M candidates\u001b[39;00m\n\u001b[1;32m     16\u001b[0m idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margpartition(\u001b[38;5;241m-\u001b[39msims, M_CANDIDATES)[:M_CANDIDATES]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── CONFIG ─────────────────────────────────────────────────────────────────────\n",
    "TRAIN_PATH        = \"../datasets/train_clicks.parquet\"\n",
    "VALID_PATH        = \"../datasets/valid_clicks.parquet\"\n",
    "USER_PROFILES_FP  = \"../datasets/user_profiles.parquet\"\n",
    "EMB_PATH          = \"../datasets/articles_embeddings.pickle\"\n",
    "META_PATH         = \"../datasets/articles_metadata.csv\"\n",
    "OUTPUT_TOP5_CSV   = \"../datasets/content_recs_top5_part1.csv\"\n",
    "OUTPUT_TOP10_CSV  = \"../datasets/content_recs_top10_part1.csv\"\n",
    "\n",
    "M_CANDIDATES = 50   # number of top candidates before filtering\n",
    "K1 = 5              # final top-5\n",
    "K2 = 10             # final top-10\n",
    "\n",
    "# ─── 1. Load training & validation clicks and user profiles ─────────────────────\n",
    "train_df        = pd.read_parquet(TRAIN_PATH, engine=\"pyarrow\")\n",
    "val_df          = pd.read_parquet(VALID_PATH, engine=\"pyarrow\")\n",
    "user_profiles   = pd.read_parquet(USER_PROFILES_FP, engine=\"pyarrow\")\n",
    "\n",
    "# Ensure timestamps are datetime\n",
    "val_df[\"click_timestamp\"] = pd.to_datetime(val_df[\"click_timestamp\"], unit=\"ms\")\n",
    "\n",
    "# ─── 2. Load article embeddings (NumPy array) and metadata ─────────────────────\n",
    "with open(EMB_PATH, \"rb\") as f:\n",
    "    embs = pickle.load(f)  # shape (N_articles, emb_dim)\n",
    "\n",
    "meta = pd.read_csv(META_PATH, usecols=[\"article_id\", \"created_at_ts\"])\n",
    "meta[\"created_at_ts\"] = pd.to_datetime(meta[\"created_at_ts\"], unit=\"ms\")\n",
    "meta = meta.sort_values(\"article_id\").reset_index(drop=True)\n",
    "\n",
    "assert embs.shape[0] == len(meta), \"Mismatch between embeddings and metadata count\"\n",
    "\n",
    "\n",
    "# Prepare arrays and maps\n",
    "article_ids = meta[\"article_id\"].to_numpy()\n",
    "emb_norm = embs / np.linalg.norm(embs, axis=1)[:, None]\n",
    "aid2pos = {aid: idx for idx, aid in enumerate(article_ids)}\n",
    "pub_map = dict(zip(meta[\"article_id\"], meta[\"created_at_ts\"]))\n",
    "\n",
    "\n",
    "# Seen articles per user (before validation click)\n",
    "seen_map    = train_df.groupby(\"user_id\")[\"click_article_id\"].apply(set).to_dict()\n",
    "# Validation click time per user\n",
    "val_time_map = val_df.set_index(\"user_id\")[\"click_timestamp\"].to_dict()\n",
    "\n",
    "# ─── 3. Compute recommendations ─────────────────────────────────────────────────\n",
    "records5  = []\n",
    "records10 = []\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for _, urow in tqdm(user_profiles.iterrows()):\n",
    "    if counter > 10_000:\n",
    "        break\n",
    "    uid = urow[\"user_id\"]\n",
    "    # a) User vector\n",
    "    uemb = urow.drop(\"user_id\").to_numpy()\n",
    "    norm = np.linalg.norm(uemb)\n",
    "    p_norm = uemb / norm if norm > 0 else uemb\n",
    "\n",
    "    # b) Cosine similarities\n",
    "    sims = emb_norm.dot(p_norm)\n",
    "\n",
    "    # c) Top-M candidates\n",
    "    idxs = np.argpartition(-sims, M_CANDIDATES)[:M_CANDIDATES]\n",
    "    top_idxs = idxs[np.argsort(-sims[idxs])]\n",
    "    cand_aids = article_ids[top_idxs]\n",
    "    cand_sims = sims[top_idxs]\n",
    "\n",
    "    # d) Filter by publication time (≤ validation click)\n",
    "    cutoff = val_time_map.get(uid, pd.Timestamp.max)\n",
    "    valid = [(aid, sim) for aid, sim in zip(cand_aids, cand_sims)\n",
    "             if pub_map.get(aid, pd.Timestamp.min) <= cutoff]\n",
    "\n",
    "    # e) Filter out already-seen\n",
    "    seen = seen_map.get(uid, set())\n",
    "    filtered = [(aid, sim) for aid, sim in valid if aid not in seen]\n",
    "\n",
    "    # f) Record top-5 and top-10\n",
    "    for rank, (aid, sim) in enumerate(filtered[:K1], start=1):\n",
    "        records5.append({\"user_id\": uid, \"rank\": rank, \"article_id\": aid, \"score\": sim})\n",
    "    for rank, (aid, sim) in enumerate(filtered[:K2], start=1):\n",
    "        records10.append({\"user_id\": uid, \"rank\": rank, \"article_id\": aid, \"score\": sim})\n",
    "    counter += 1\n",
    "# ─── 4. Save results ────────────────────────────────────────────────────────────\n",
    "pd.DataFrame(records5).to_csv(OUTPUT_TOP5_CSV, index=False)\n",
    "pd.DataFrame(records10).to_csv(OUTPUT_TOP10_CSV, index=False)\n",
    "\n",
    "print(f\"Saved top-{K1} content-based recs → {OUTPUT_TOP5_CSV}\")\n",
    "print(f\"Saved top-{K2} content-based recs → {OUTPUT_TOP10_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "gather": {
     "logged": 1745832933688
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65536, 251)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_profiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "gather": {
     "logged": 1745833070630
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 65536 user profiles → '../datasets/user_profiles.parquet'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Top K = 10 articles' computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "gather": {
     "logged": 1745836285227
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# ─── CONFIG ─────────────────────────────────────────────────────────────────────\n",
    "USER_PROFILES_PATH = \"../datasets/user_profiles.parquet\"\n",
    "EMB_PATH           = \"../datasets/articles_embeddings.pickle\"\n",
    "TRAIN_PATH         = \"../datasets/train_clicks.parquet\"\n",
    "OUTPUT_PATH        = \"../datasets/user_recs_content.parquet\"\n",
    "M_CANDIDATES       = 50  # number of neighbors to retrieve before filtering\n",
    "K_RECOMMEND        = 10  # final number of recommendations per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "gather": {
     "logged": 1745742522012
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ─── 1. Load user profiles and article embeddings ───────────────────────────────\n",
    "user_profiles = pd.read_parquet(USER_PROFILES_PATH, engine=\"pyarrow\")\n",
    "\n",
    "with open(EMB_PATH, \"rb\") as f:\n",
    "    embs = pickle.load(f)  # numpy array (num_articles, emb_dim)\n",
    "# Load metadata to align article IDs\n",
    "meta = pd.read_csv(\"articles_metadata.csv\", usecols=[\"article_id\"]).sort_values(\"article_id\").reset_index(drop=True)\n",
    "assert embs.shape[0] == len(meta), \"Embedding count and metadata count mismatch\"\n",
    "\n",
    "# Build embeddings DataFrame: index=article_id, columns emb_0...emb_dim\n",
    "emb_dim = embs.shape[1]\n",
    "emb_cols = [f\"emb_{i}\" for i in range(emb_dim)]\n",
    "emb_df = pd.DataFrame(embs, index=meta[\"article_id\"], columns=emb_cols)\n",
    "emb_df.index.name = \"article_id\"\n",
    "\n",
    "# ─── 2. Fit NearestNeighbors on article embeddings ───────────────────────────────\n",
    "nn_model = NearestNeighbors(n_neighbors=M_CANDIDATES, metric=\"cosine\")\n",
    "nn_model.fit(emb_df.values)\n",
    "article_ids = emb_df.index.to_numpy()\n",
    "\n",
    "# ─── 3. Load train clicks to get seen articles per user ─────────────────────────\n",
    "train_df = pd.read_parquet(TRAIN_PATH, engine=\"pyarrow\")\n",
    "seen_articles = train_df.groupby(\"user_id\")[\"click_article_id\"].apply(set).to_dict()\n",
    "\n",
    "# ─── 4. Generate and store top-K recs for each user ─────────────────────────────\n",
    "records = []\n",
    "for idx, row in user_profiles.iterrows():\n",
    "    uid = row[\"user_id\"]\n",
    "    profile_vec = row[emb_cols].to_numpy().reshape(1, -1)\n",
    "    dists, idxs = nn_model.kneighbors(profile_vec)\n",
    "    sims = 1 - dists.flatten()\n",
    "    candidates = article_ids[idxs.flatten()]\n",
    "    \n",
    "    # Filter out seen articles\n",
    "    seen = seen_articles.get(uid, set())\n",
    "    filtered = [(aid, sim) for aid, sim in zip(candidates, sims) if aid not in seen]\n",
    "    \n",
    "    # Take top-K after filtering\n",
    "    for rank, (aid, sim) in enumerate(filtered[:K_RECOMMEND], start=1):\n",
    "        records.append({\n",
    "            \"user_id\": uid,\n",
    "            \"recommendation_rank\": rank,\n",
    "            \"article_id\": aid,\n",
    "            \"score\": sim\n",
    "        })\n",
    "\n",
    "# Build DataFrame and save\n",
    "recs_df = pd.DataFrame(records)\n",
    "recs_df.to_parquet(OUTPUT_PATH, engine=\"pyarrow\", index=False)\n",
    "print(f\"Saved content-based recommendations for {recs_df['user_id'].nunique()} users → '{OUTPUT_PATH}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1745829810440
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gather": {
     "logged": 1745740620240
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python (p9-recsys)",
   "language": "python",
   "name": "p9-recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
