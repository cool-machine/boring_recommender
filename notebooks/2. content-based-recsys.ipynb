{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 2 Content Based Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 14:42:50 WARN Utils: Your hostname, p962cnts8crs64g128g resolves to a loopback address: 127.0.0.1; using 10.0.0.4 instead (on interface eth0)\n",
      "25/04/30 14:42:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/30 14:42:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, LongType, IntegerType, DoubleType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# ─── 1) Start Spark with console‐progress enabled ────────────────────────────────\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ContentBasedSpark\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# ─── 2) Config & broadcast data ─────────────────────────────────────────────────\n",
    "M_CANDIDATES      = 50\n",
    "K1, K2            = 5, 10\n",
    "EMB_FP            = \"../datasets/articles_embeddings.pickle\"\n",
    "META_FP           = \"../datasets/articles_metadata.csv\"\n",
    "TRAIN_FP          = \"../datasets/train_clicks.parquet\"\n",
    "VALID_FP          = \"../datasets/valid_clicks.parquet\"\n",
    "UP_FP             = \"../datasets/user_profiles.parquet\"\n",
    "\n",
    "# Load embeddings & normalize\n",
    "with open(EMB_FP, \"rb\") as f:\n",
    "    embs = pickle.load(f)\n",
    "emb_norm = embs / np.linalg.norm(embs, axis=1)[:, None]\n",
    "\n",
    "# Load metadata and build maps\n",
    "meta = pd.read_csv(META_FP, usecols=[\"article_id\",\"created_at_ts\"])\n",
    "meta[\"created_at_ts\"] = pd.to_datetime(meta[\"created_at_ts\"], unit=\"ms\")\n",
    "meta = meta.sort_values(\"article_id\").reset_index(drop=True)\n",
    "article_ids = meta[\"article_id\"].to_numpy()\n",
    "pub_map     = dict(zip(meta[\"article_id\"], meta[\"created_at_ts\"]))\n",
    "\n",
    "# Load clicks and build seen/val–time maps\n",
    "train_pd     = pd.read_parquet(TRAIN_FP, engine=\"pyarrow\")\n",
    "val_pd       = pd.read_parquet(VALID_FP, engine=\"pyarrow\")\n",
    "val_pd[\"click_timestamp\"] = pd.to_datetime(val_pd[\"click_timestamp\"], unit=\"ms\")\n",
    "seen_map     = train_pd.groupby(\"user_id\")[\"click_article_id\"].apply(set).to_dict()\n",
    "val_time_map = val_pd.set_index(\"user_id\")[\"click_timestamp\"].to_dict()\n",
    "\n",
    "# Broadcast to executors\n",
    "b_article_ids  = sc.broadcast(article_ids)\n",
    "b_emb_norm     = sc.broadcast(emb_norm)\n",
    "b_pub_map      = sc.broadcast(pub_map)\n",
    "b_seen_map     = sc.broadcast(seen_map)\n",
    "b_val_time_map = sc.broadcast(val_time_map)\n",
    "\n",
    "# Load user profiles\n",
    "up_df = spark.read.parquet(UP_FP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, IntegerType, DoubleType\n",
    "\n",
    "# Define output schema\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\",    LongType(),    False),\n",
    "    StructField(\"rank\",       IntegerType(), False),\n",
    "    StructField(\"article_id\", LongType(),    False),\n",
    "    StructField(\"score\",      DoubleType(),  False),\n",
    "])\n",
    "\n",
    "def recommend_batch(pdf_iter):\n",
    "    for pdf in pdf_iter:\n",
    "        recs = {\"user_id\":[], \"rank\":[], \"article_id\":[], \"score\":[]}\n",
    "        emb_cols = [c for c in pdf.columns if c != \"user_id\"]\n",
    "        for row in pdf.itertuples(index=False):\n",
    "            uid = int(row.user_id)\n",
    "            uemb = np.array([getattr(row,c) for c in emb_cols], dtype=float)\n",
    "            norm = np.linalg.norm(uemb)\n",
    "            u_norm = uemb/norm if norm>0 else uemb\n",
    "\n",
    "            sims = b_emb_norm.value.dot(u_norm)\n",
    "            idxs = np.argpartition(-sims, M_CANDIDATES)[:M_CANDIDATES]\n",
    "            idxs = idxs[np.argsort(-sims[idxs])]\n",
    "            aids = b_article_ids.value[idxs]\n",
    "            ss  = sims[idxs]\n",
    "\n",
    "            cutoff = b_val_time_map.value.get(uid, pd.Timestamp.max)\n",
    "            valid = [(a,s) for a,s in zip(aids,ss)\n",
    "                     if b_pub_map.value.get(a,pd.Timestamp.min) <= cutoff]\n",
    "\n",
    "            seen = b_seen_map.value.get(uid, set())\n",
    "            filtered = [(a,s) for a,s in valid if a not in seen]\n",
    "\n",
    "            for rank, (a,s) in enumerate(filtered[:K1], start=1):\n",
    "                recs[\"user_id\"].append(uid)\n",
    "                recs[\"rank\"].append(rank)\n",
    "                recs[\"article_id\"].append(int(a))\n",
    "                recs[\"score\"].append(float(s))\n",
    "            for rank, (a,s) in enumerate(filtered[:K2], start=1):\n",
    "                recs[\"user_id\"].append(uid)\n",
    "                recs[\"rank\"].append(rank)\n",
    "                recs[\"article_id\"].append(int(a))\n",
    "                recs[\"score\"].append(float(s))\n",
    "        \n",
    "        yield pd.DataFrame(recs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 14:44:05 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 1:===================================================>       (7 + 1) / 8]\r"
     ]
    }
   ],
   "source": [
    "# Apply the UDF in parallel\n",
    "result = up_df.mapInPandas(recommend_batch, schema=schema)\n",
    "\n",
    "# Write out your top-5 and top-10\n",
    "result.filter(\"rank <= 5\") \\\n",
    "      .write.mode(\"overwrite\") \\\n",
    "      .csv(\"../datasets/content_recs_top5\", header=True)\n",
    "\n",
    "result.filter(\"rank <= 10\") \\\n",
    "      .write.mode(\"overwrite\") \\\n",
    "      .csv(\"../datasets/content_recs_top10\", header=True)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Read recs into pandas (they’re small: one row per user × K)\n",
    "\n",
    "def load_spark_csvs(output_dir: str) -> pd.DataFrame:\n",
    "    base = Path(output_dir)\n",
    "    parts = [\n",
    "        p for p in base.rglob(\"part-*.csv\")\n",
    "        if not any(seg.startswith(\"attempt_\") for seg in p.parts)\n",
    "    ]\n",
    "    if not parts:\n",
    "        raise FileNotFoundError(f\"No committed CSV shards found in {output_dir}\")\n",
    "    return pd.concat((pd.read_csv(p) for p in parts), ignore_index=True)\n",
    "\n",
    "# load Top-5 (2 shards at root) and Top-10 (1 shard under task_…)\n",
    "rec5_df  = load_spark_csvs(\"../datasets/content_recs_top5\")\n",
    "rec10_df = load_spark_csvs(\"../datasets/content_recs_top10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3347/2457221324.py:3: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  top10 = rec10_df.groupby(\"user_id\")[\"article_id\"].apply(list).rename(\"recs10\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     0\n",
      "Recall@5      0.000061\n",
      "Precision@5   0.000012\n",
      "MRR@5         0.000043\n",
      "Recall@10     0.000000\n",
      "Precision@10  0.000000\n",
      "MRR@10        0.000000\n"
     ]
    }
   ],
   "source": [
    "# 2) Pivot to one row per user\n",
    "top5  = rec5_df .groupby(\"user_id\")[\"article_id\"].apply(list).rename(\"recs5\")\n",
    "top10 = rec10_df.groupby(\"user_id\")[\"article_id\"].apply(list).rename(\"recs10\")\n",
    "\n",
    "# 3) Load validation clicks\n",
    "val = pd.read_parquet(\"../datasets/valid_clicks.parquet\", engine=\"pyarrow\")\n",
    "val = val.rename(columns={\"click_article_id\": \"true_click\"}).set_index(\"user_id\")\n",
    "\n",
    "# 4) Merge\n",
    "eval_df = val.join(top5,  how=\"left\").join(top10, how=\"left\")\n",
    "\n",
    "# 5) Compute metrics\n",
    "def compute_metrics(row, K):\n",
    "    recs = row[f\"recs{K}\"]\n",
    "    true = row[\"true_click\"]\n",
    "    if not isinstance(recs, list): \n",
    "        return pd.Series({\"hit\": 0, \"prec\": 0.0, \"rr\": 0.0})\n",
    "    hit = int(true in recs)\n",
    "    prec = hit / K\n",
    "    # reciprocal rank: 1/(position), or 0 if not present\n",
    "    rr = 1.0 / (recs.index(true) + 1) if hit else 0.0\n",
    "    return pd.Series({\"hit\": hit, \"prec\": prec, \"rr\": rr})\n",
    "\n",
    "# apply for K=5 and K=10\n",
    "m5  = eval_df.apply(lambda r: compute_metrics(r, 5),  axis=1)\n",
    "m10 = eval_df.apply(lambda r: compute_metrics(r, 10), axis=1)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Recall@5\":    m5[\"hit\"].mean(),\n",
    "    \"Precision@5\": m5[\"prec\"].mean(),\n",
    "    \"MRR@5\":       m5[\"rr\"].mean(),\n",
    "    \"Recall@10\":   m10[\"hit\"].mean(),\n",
    "    \"Precision@10\":m10[\"prec\"].mean(),\n",
    "    \"MRR@10\":      m10[\"rr\"].mean(),\n",
    "}, index=[0])\n",
    "\n",
    "print(results.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, LongType, IntegerType, DoubleType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# ─── 1) Start Spark with console‐progress enabled ────────────────────────────────\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ContentBasedSpark\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# ─── 2) Config & broadcast data ─────────────────────────────────────────────────\n",
    "M_CANDIDATES      = 50\n",
    "K1, K2            = 5, 10\n",
    "EMB_FP            = \"../datasets/articles_embeddings.pickle\"\n",
    "META_FP           = \"../datasets/articles_metadata.csv\"\n",
    "TRAIN_FP          = \"../datasets/train_clicks.parquet\"\n",
    "VALID_FP          = \"../datasets/valid_clicks.parquet\"\n",
    "UP_FP             = \"../datasets/user_profiles.parquet\"\n",
    "\n",
    "# Load embeddings & normalize\n",
    "with open(EMB_FP, \"rb\") as f:\n",
    "    embs = pickle.load(f)\n",
    "emb_norm = embs / np.linalg.norm(embs, axis=1)[:, None]\n",
    "\n",
    "# Load metadata and build maps\n",
    "meta = pd.read_csv(META_FP, usecols=[\"article_id\",\"created_at_ts\"])\n",
    "meta[\"created_at_ts\"] = pd.to_datetime(meta[\"created_at_ts\"], unit=\"ms\")\n",
    "meta = meta.sort_values(\"article_id\").reset_index(drop=True)\n",
    "article_ids = meta[\"article_id\"].to_numpy()\n",
    "pub_map     = dict(zip(meta[\"article_id\"], meta[\"created_at_ts\"]))\n",
    "\n",
    "# Load clicks and build seen/val–time maps\n",
    "train_pd     = pd.read_parquet(TRAIN_FP, engine=\"pyarrow\")\n",
    "val_pd       = pd.read_parquet(VALID_FP, engine=\"pyarrow\")\n",
    "val_pd[\"click_timestamp\"] = pd.to_datetime(val_pd[\"click_timestamp\"], unit=\"ms\")\n",
    "seen_map     = train_pd.groupby(\"user_id\")[\"click_article_id\"].apply(set).to_dict()\n",
    "val_time_map = val_pd.set_index(\"user_id\")[\"click_timestamp\"].to_dict()\n",
    "\n",
    "# Broadcast to executors\n",
    "b_article_ids  = sc.broadcast(article_ids)\n",
    "b_emb_norm     = sc.broadcast(emb_norm)\n",
    "b_pub_map      = sc.broadcast(pub_map)\n",
    "b_seen_map     = sc.broadcast(seen_map)\n",
    "b_val_time_map = sc.broadcast(val_time_map)\n",
    "\n",
    "# Load user profiles\n",
    "up_df = spark.read.parquet(UP_FP)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, LongType, IntegerType, DoubleType\n",
    "\n",
    "# Define output schema\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\",    LongType(),    False),\n",
    "    StructField(\"rank\",       IntegerType(), False),\n",
    "    StructField(\"article_id\", LongType(),    False),\n",
    "    StructField(\"score\",      DoubleType(),  False),\n",
    "])\n",
    "\n",
    "def recommend_batch(pdf_iter):\n",
    "    for pdf in pdf_iter:\n",
    "        recs = {\"user_id\":[], \"rank\":[], \"article_id\":[], \"score\":[]}\n",
    "        emb_cols = [c for c in pdf.columns if c != \"user_id\"]\n",
    "        for row in pdf.itertuples(index=False):\n",
    "            uid = int(row.user_id)\n",
    "            uemb = np.array([getattr(row,c) for c in emb_cols], dtype=float)\n",
    "            norm = np.linalg.norm(uemb)\n",
    "            u_norm = uemb/norm if norm>0 else uemb\n",
    "\n",
    "            sims = b_emb_norm.value.dot(u_norm)\n",
    "            idxs = np.argpartition(-sims, M_CANDIDATES)[:M_CANDIDATES]\n",
    "            idxs = idxs[np.argsort(-sims[idxs])]\n",
    "            aids = b_article_ids.value[idxs]\n",
    "            ss  = sims[idxs]\n",
    "\n",
    "            cutoff = b_val_time_map.value.get(uid, pd.Timestamp.max)\n",
    "            valid = [(a,s) for a,s in zip(aids,ss)\n",
    "                     if b_pub_map.value.get(a,pd.Timestamp.min) <= cutoff]\n",
    "\n",
    "            seen = b_seen_map.value.get(uid, set())\n",
    "            filtered = [(a,s) for a,s in valid if a not in seen]\n",
    "\n",
    "            for rank, (a,s) in enumerate(filtered[:K1], start=1):\n",
    "                recs[\"user_id\"].append(uid)\n",
    "                recs[\"rank\"].append(rank)\n",
    "                recs[\"article_id\"].append(int(a))\n",
    "                recs[\"score\"].append(float(s))\n",
    "            for rank, (a,s) in enumerate(filtered[:K2], start=1):\n",
    "                recs[\"user_id\"].append(uid)\n",
    "                recs[\"rank\"].append(rank)\n",
    "                recs[\"article_id\"].append(int(a))\n",
    "                recs[\"score\"].append(float(s))\n",
    "        \n",
    "        yield pd.DataFrame(recs)\n",
    "        \n",
    "        \n",
    "# Apply the UDF in parallel\n",
    "result = up_df.mapInPandas(recommend_batch, schema=schema)\n",
    "\n",
    "# Write out your top-5 and top-10\n",
    "result.filter(\"rank <= 5\") \\\n",
    "      .write.mode(\"overwrite\") \\\n",
    "      .csv(\"../datasets/content_recs_top5\", header=True)\n",
    "\n",
    "result.filter(\"rank <= 10\") \\\n",
    "      .write.mode(\"overwrite\") \\\n",
    "      .csv(\"../datasets/content_recs_top10\", header=True)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "gather": {
     "logged": 1745836285227
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# ─── CONFIG ─────────────────────────────────────────────────────────────────────\n",
    "USER_PROFILES_PATH = \"../datasets/user_profiles.parquet\"\n",
    "EMB_PATH           = \"../datasets/articles_embeddings.pickle\"\n",
    "TRAIN_PATH         = \"../datasets/train_clicks.parquet\"\n",
    "OUTPUT_PATH        = \"../datasets/user_recs_content.parquet\"\n",
    "M_CANDIDATES       = 50  # number of neighbors to retrieve before filtering\n",
    "K_RECOMMEND        = 10  # final number of recommendations per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "gather": {
     "logged": 1745742522012
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ─── 1. Load user profiles and article embeddings ───────────────────────────────\n",
    "user_profiles = pd.read_parquet(USER_PROFILES_PATH, engine=\"pyarrow\")\n",
    "\n",
    "with open(EMB_PATH, \"rb\") as f:\n",
    "    embs = pickle.load(f)  # numpy array (num_articles, emb_dim)\n",
    "# Load metadata to align article IDs\n",
    "meta = pd.read_csv(\"articles_metadata.csv\", usecols=[\"article_id\"]).sort_values(\"article_id\").reset_index(drop=True)\n",
    "assert embs.shape[0] == len(meta), \"Embedding count and metadata count mismatch\"\n",
    "\n",
    "# Build embeddings DataFrame: index=article_id, columns emb_0...emb_dim\n",
    "emb_dim = embs.shape[1]\n",
    "emb_cols = [f\"emb_{i}\" for i in range(emb_dim)]\n",
    "emb_df = pd.DataFrame(embs, index=meta[\"article_id\"], columns=emb_cols)\n",
    "emb_df.index.name = \"article_id\"\n",
    "\n",
    "# ─── 2. Fit NearestNeighbors on article embeddings ───────────────────────────────\n",
    "nn_model = NearestNeighbors(n_neighbors=M_CANDIDATES, metric=\"cosine\")\n",
    "nn_model.fit(emb_df.values)\n",
    "article_ids = emb_df.index.to_numpy()\n",
    "\n",
    "# ─── 3. Load train clicks to get seen articles per user ─────────────────────────\n",
    "train_df = pd.read_parquet(TRAIN_PATH, engine=\"pyarrow\")\n",
    "seen_articles = train_df.groupby(\"user_id\")[\"click_article_id\"].apply(set).to_dict()\n",
    "\n",
    "# ─── 4. Generate and store top-K recs for each user ─────────────────────────────\n",
    "records = []\n",
    "for idx, row in user_profiles.iterrows():\n",
    "    uid = row[\"user_id\"]\n",
    "    profile_vec = row[emb_cols].to_numpy().reshape(1, -1)\n",
    "    dists, idxs = nn_model.kneighbors(profile_vec)\n",
    "    sims = 1 - dists.flatten()\n",
    "    candidates = article_ids[idxs.flatten()]\n",
    "    \n",
    "    # Filter out seen articles\n",
    "    seen = seen_articles.get(uid, set())\n",
    "    filtered = [(aid, sim) for aid, sim in zip(candidates, sims) if aid not in seen]\n",
    "    \n",
    "    # Take top-K after filtering\n",
    "    for rank, (aid, sim) in enumerate(filtered[:K_RECOMMEND], start=1):\n",
    "        records.append({\n",
    "            \"user_id\": uid,\n",
    "            \"recommendation_rank\": rank,\n",
    "            \"article_id\": aid,\n",
    "            \"score\": sim\n",
    "        })\n",
    "\n",
    "# Build DataFrame and save\n",
    "recs_df = pd.DataFrame(records)\n",
    "recs_df.to_parquet(OUTPUT_PATH, engine=\"pyarrow\", index=False)\n",
    "print(f\"Saved content-based recommendations for {recs_df['user_id'].nunique()} users → '{OUTPUT_PATH}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1745829810440
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gather": {
     "logged": 1745740620240
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python (p9-recsys)",
   "language": "python",
   "name": "p9-recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
